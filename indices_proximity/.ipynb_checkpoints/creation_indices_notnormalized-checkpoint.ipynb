{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33804bee",
   "metadata": {},
   "source": [
    "# Creation of the indices of proximity: not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc659d5",
   "metadata": {},
   "source": [
    "Importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4aa0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "import nltk\n",
    "import missingno as msno\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import statistics\n",
    "import math\n",
    "import time\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "\n",
    "# enabling Jupyter Lab to include the output of our plots directly in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# used to avoid blurry output plots in Jupyter Notebooks\n",
    "%config InLineBackend.figure_format = \"retina\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed1e99",
   "metadata": {},
   "source": [
    "Downloading the files I will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577cf5f",
   "metadata": {},
   "source": [
    "This is an extended version of df_full_cleaned, with all information I need for my analysises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05723136",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9056\\3340115243.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minfile_df_full_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../exploratory_analysis/data_exploratory_analysis/df_full_extended'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_full_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile_df_full_extended\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minfile_df_full_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infile_df_full_extended = open('../exploratory_analysis/data_exploratory_analysis/df_full_extended','rb')\n",
    "df_full_extended = pickle.load(infile_df_full_extended)\n",
    "infile_df_full_extended.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77653b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_df_full = open('../creation_data_and_variables/data_creation_variables/df_full','rb')\n",
    "df_full = pickle.load(infile_df_full)\n",
    "infile_df_full.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d658c74",
   "metadata": {},
   "source": [
    "# **Part 1 -- Processing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de93acf",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "1.1 Preparation dataset \"df_full_extended\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073fd43",
   "metadata": {},
   "source": [
    "I change the type of variable of my database \"df_full_extended\" for my further computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8667464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_extended['cosine_similarity']=df_full_extended['cosine_similarity'].astype(float)\n",
    "df_full_extended['score_concepts']=df_full_extended['score_concepts'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e9c32",
   "metadata": {},
   "source": [
    "I check the type of variables I have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_extended.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efaa42",
   "metadata": {},
   "source": [
    "Everything is fine just as expected. Now I compute the mean of my numerical variables, over months and years, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_extended.groupby(['year','month']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aba4f",
   "metadata": {},
   "source": [
    "I now want to put the score of attribution to concepts as columns instead of one column for each paper.\n",
    "I create an auxiliary dataframe to reach this aim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp = df_full_extended[['paper','concepts','score_concepts']]\n",
    "dfhelp=dfhelp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp=dfhelp.set_index(['paper','concepts'])['score_concepts'].unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d61fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474b83b",
   "metadata": {},
   "source": [
    "Now I want to merge this dataframe with another frame, whose columns 'concepts', 'score_concepts', 'referenced_works' where deleted such that I obtain one big dataframe, with the concepts as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftomerge = df_full_extended.drop(['concepts','score_concepts','referenced_works'], axis=1)\n",
    "dftomerge = dftomerge.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b12878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfintermed = pd.merge(dftomerge,dfhelp, on='paper', how='inner')\n",
    "dfintermed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7be416",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "I've go everything except the column 'referenced_works'. Again, I do a merge to obtain the final dataframe I am aiming at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbf696",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfref=df_full_extended[['paper','referenced_works']]\n",
    "dfref=dfref.drop_duplicates(subset=['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b31fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfall = pd.merge(dfref, dfintermed, on='paper', how='inner')\n",
    "dfall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0254db5",
   "metadata": {},
   "source": [
    "I explode the column 'referenced_works', in order to have the complete dataframe I am aiming at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = dfall.explode('referenced_works')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2221392",
   "metadata": {},
   "source": [
    "Now I do have my final dataframe, I will use to compute my indices of proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b512d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3c1d0",
   "metadata": {},
   "source": [
    "Last I save this dataframe, since it could be useful, not to have to do the computations again each time, I am interest in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4878a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_pickle('data_indices/df_computations_indices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0997f33",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "1.2 Preparation auxiliary dataset for references\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a002017",
   "metadata": {},
   "source": [
    "Now I want to do the same with df_full to use it as source of information for the cited papers. In fact, I deleted many papers from the set of papers I am studying, because there were no referenced works, several ids for the same paper, etc. All these papers or ids are still part of the set of referenced_works. For this reason, I need the full dataset to have information about these papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adccd99",
   "metadata": {},
   "source": [
    "I rename one column, to make everything more uniform among my datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.rename(columns={\"id\": \"paper\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a064cc6",
   "metadata": {},
   "source": [
    "Again, I will change my dataset, putting the concepts as columns having directly the score of attribution whithin themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp = df_full[['paper','concepts','score_concepts']]\n",
    "dfhelp=dfhelp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp['score_concepts']=dfhelp['score_concepts'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad161746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp=dfhelp.set_index(['paper','concepts'])['score_concepts'].unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftomerge = df_full.drop(['concepts','score_concepts'], axis=1)\n",
    "dftomerge = dftomerge.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auxiliary_ref = pd.merge(dftomerge,dfhelp, on='paper', how='inner')\n",
    "df_auxiliary_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33b870",
   "metadata": {},
   "source": [
    "This is now done. I check how many papers are affiliated to no technologies at all. They will not count in my computations, since I can not attribute them to any technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b817b60",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "1.3 Checking papers attributed to no technologies at all\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_concepts= ['Authentication protocole','Biometrics','Blockchain','Digital rights management'\n",
    ",'Digital signature','Distributed algorithm','Electronic voting','Functional encryption',\n",
    "'Hardware acceleration','Hardware security module','Hash function','Homomorphic encryption','Identity management',\n",
    "'Key management','Link encryption','Post-quantum cryptography','Public-key cryptography','Quantum key distribution',\n",
    "'Quantum cryptography','Random number generation','Symmetric-key algorithm','Threshold cryptosystem',\n",
    "'Trusted Computing','Tunneling protocol','Zero-knowlegde proof']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myinfodataframe = dfhelp\n",
    "for tech in list_concepts:\n",
    "    myinfodataframe = myinfodataframe.loc[myinfodataframe[tech]==0]\n",
    "myinfodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_weird_papers = list(set(myinfodataframe.paper.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44aaa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are '+str(len(list_weird_papers))+' papers attributed to no technologies at all.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60554f9d",
   "metadata": {},
   "source": [
    "I do have all the data I need. I define the functions I will use and then, I will compute the indices of proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442dda8b",
   "metadata": {},
   "source": [
    "# **Part 2 -- Function for the computation of the indices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163134db",
   "metadata": {},
   "source": [
    "For each author and for each month, the function 'importance_author_intech' takes its hindex (incremental or not, based on the condition), the average attribution to technology 1 and 2 of the papers published during this month, and the average number of times this author appear in t1 and t2 during this month.\\\n",
    "This function returns 'hindex * attribution_to_t1_t2 * average_time_in_t1_2', which should represent the importance of the author for this link of technologies during this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_author_t1_t2(author,df,tech1,tech2,condition):\n",
    "    myinfo = df.loc[df['author']==author]\n",
    "    \n",
    "    dftech1 =myinfo.loc[myinfo[tech1]!=0]\n",
    "    dftech2 =myinfo.loc[myinfo[tech2]!=0]\n",
    "    \n",
    "    #to compute the number of times this author published a paper related to tech 1 or tech 2\n",
    "    times_author_in_t1 = len(list(set(dftech1.paper.tolist())))\n",
    "    times_author_in_t2 = len(list(set(dftech2.paper.tolist())))\n",
    "    \n",
    "    attribution_tech1= dftech1[tech1].tolist()\n",
    "    attribution_tech2= dftech2[tech2].tolist()\n",
    "    \n",
    "    if condition == 'incremental':\n",
    "        hindex = myinfo.monthly_H_index_incremental.tolist()[0]+1\n",
    "    if condition == 'nonincremental':\n",
    "        hindex = myinfo.monthly_H_index_notincremental.tolist()[0]+1\n",
    "    \n",
    "    attribution_to_t1_t2 = (np.mean(attribution_tech1)+np.mean(attribution_tech2))/2\n",
    "    \n",
    "    average_time_in_t1_2 = (times_author_in_t1+times_author_in_t2)/2\n",
    "    \n",
    "    element_sum = hindex*attribution_to_t1_t2*average_time_in_t1_2\n",
    "    \n",
    "    return element_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_author_t1(author,df,tech1,condition):\n",
    "    myinfo = df.loc[df['author']==author]\n",
    "    dftech1 =myinfo.loc[myinfo[tech1]!=0]\n",
    "    \n",
    "    #to compute the number of times this author published a paper related to tech 1 or tech 2\n",
    "    attribution_tech1= dftech1[tech1].tolist()\n",
    "    \n",
    "    if condition == 'incremental':\n",
    "        hindex = myinfo.monthly_H_index_incremental.tolist()[0]+1\n",
    "    if condition == 'nonincremental':\n",
    "        hindex = myinfo.monthly_H_index_notincremental.tolist()[0]+1\n",
    "    \n",
    "    attribution_to_t1 = np.mean(attribution_tech1)\n",
    "    times_author_in_t1 = len(list(set(dftech1.paper.tolist())))\n",
    "    \n",
    "    element_sum = hindex*attribution_to_t1*times_author_in_t1\n",
    "    \n",
    "    return element_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a81dfc",
   "metadata": {},
   "source": [
    "In very similar fashion as done for authors, for each keyword during each month, the function 'importance_keywords_intech' takes the average cosine_similarity of the keyword of all the times it appears in t1 and t2 during this month, the average attribution to technology 1 and 2 of the papers published during this month where the keyword appears as a keyword, and the average number of times this keyword appear in t1 and t2 during this month.\\\n",
    "This function returns 'average_cosine_similarity * attribution_to_t1_t2 * average_time_in_t1_2', which should represent the importance of the keyword for this link of technologies during this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_keywords_t1_t2(keyword,df,tech1,tech2):\n",
    "    myinfo = df.loc[df['keyword']==keyword]\n",
    "    \n",
    "    dftech1 =myinfo.loc[myinfo[tech1]!=0]\n",
    "    dftech2 =myinfo.loc[myinfo[tech2]!=0]\n",
    "    \n",
    "    times_keywords_in_t1 = len(list(set(dftech1.paper.tolist())))\n",
    "    times_keywords_in_t2 = len(list(set(dftech2.paper.tolist())))\n",
    "    \n",
    "    attribution_tech1= dftech1[tech1].tolist()\n",
    "    attribution_tech2= dftech2[tech2].tolist()\n",
    "    \n",
    "    cosine_similarity_tech1= dftech1.cosine_similarity.tolist()\n",
    "    cosine_similarity_tech2= dftech2.cosine_similarity.tolist()\n",
    "    \n",
    "    average_cosine_similarity = (np.mean(cosine_similarity_tech1)+np.mean(cosine_similarity_tech2))/2\n",
    "    attribution_to_t1_t2 = (np.mean(attribution_tech1)+np.mean(attribution_tech2))/2\n",
    "    average_time_in_t1_2 = (times_keywords_in_t1+times_keywords_in_t2)/2\n",
    "    \n",
    "    element_sum = average_cosine_similarity*attribution_to_t1_t2*average_time_in_t1_2\n",
    "    \n",
    "    return element_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_keywords_t1(keyword,df,tech1):\n",
    "    dfkey = df.loc[df['keyword']==keyword]\n",
    "\n",
    "    attribution_tech1= dfkey[tech1].tolist()\n",
    "    cosine_similarity_tech1= dfkey.cosine_similarity.tolist()\n",
    "\n",
    "    average_cosine_similarity = np.mean(cosine_similarity_tech1)\n",
    "    attribution_to_t1 = np.mean(attribution_tech1)\n",
    "    times_keywords_in_t1 = len(list(set(dfkey.paper.tolist())))\n",
    "    \n",
    "    element_sum = average_cosine_similarity*attribution_to_t1*times_keywords_in_t1\n",
    "    \n",
    "    return element_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daff69",
   "metadata": {},
   "source": [
    "We now come to the part about citations. This is sligthly more difficult and we have 4 functions in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bbbed",
   "metadata": {},
   "source": [
    "The function below computes for a paper and a technology 1 and 2, the sum of the mean of attribution to technology 1 and 2 for all the referenced works related to 2 with respect to my paper which is attributed to technology 1.\\\n",
    "In other words, it gives: sum((attr_to_t1 + attr_to_t2_ref_x)/2) for all ref_x in referenced_works of my paper, where attr_to_t1 is the score of attribution of my paper to t1 and attr_to_t2_ref_x is the score of attribution to t2 of each referenced work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c719d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_cit_tech(paper,df,tech1,tech2,df_ref_help):\n",
    "  \n",
    "    myinfo = df.loc[df['paper']==paper]\n",
    "\n",
    "    attribution_tech1= myinfo[tech1].tolist()[0]\n",
    "    \n",
    "    referenced_works=list(set(myinfo['referenced_works'].tolist()))\n",
    "    \n",
    "    list_component_sum_ref = list(map(lambda x: info_ref_tech(x,df_ref_help,attribution_tech1,tech2), referenced_works))\n",
    "    \n",
    "    return sum(list_component_sum_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb92e33",
   "metadata": {},
   "source": [
    "This function below is an auxiliary function. For a referenced paper 'ref', the score of attribution to technology 1 of the paper which has 'ref' as reference and a technology 'tech2', this function gives (attr_to_t1 + attr_to_t2_ref_x)/2 as explained above, if the referenced work is connected to 'tech2' and zero either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34970aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_ref_tech(ref,df,attribution_paper_t1,tech2):\n",
    "    myinforef = df.loc[df['paper']==str(ref)]\n",
    "    #then I have a paper not related to my research\n",
    "    if len(myinforef)==0:\n",
    "        attribution_t1_t2 =0\n",
    "    else:\n",
    "        att_to_t2 = myinforef[tech2].tolist()[0]\n",
    "        if att_to_t2!=0:\n",
    "            attribution_t1_t2 = (attribution_paper_t1+att_to_t2)/2\n",
    "        else:\n",
    "            attribution_t1_t2 = 0\n",
    "    return attribution_t1_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fa1c4",
   "metadata": {},
   "source": [
    "__Definition main functions for the computation of my indices__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96005a0b",
   "metadata": {},
   "source": [
    "I now define the function that will properly compute the indices of proximity, for the given dataframes.\n",
    "All this function do the same, they compute a list of indices of proximity for all combination of technologies during a specific month taking the subdataframe containing all required information for the month during the specific year. They return then this list of indices of proximity, for each month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dedc25",
   "metadata": {},
   "source": [
    "The function below computes the index of proximity regarding keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_keywords(dfinfos,listconcepts):\n",
    "    \n",
    "    start = time.time()\n",
    "    listindices= []\n",
    "    \n",
    "    for tech1 in list_concepts:\n",
    "        for tech2 in list_concepts:\n",
    "\n",
    "            # variables for my computations\n",
    "    \n",
    "            dftech1 =dfinfos.loc[dfinfos[tech1]!=0]\n",
    "            dftech2 =dfinfos.loc[dfinfos[tech2]!=0]\n",
    "    \n",
    "            if len(dftech2) == 0 or len(dftech1)==0:\n",
    "            # this means we can not compute anything because we have no data\n",
    "            # we decide to let it blank.\n",
    "                index_keywords=np.nan\n",
    "            else:\n",
    "    \n",
    "                # keywords\n",
    "    \n",
    "                keywords_tech1 = dftech1.keyword.tolist()\n",
    "                keywords_tech2 = dftech2.keyword.tolist()\n",
    "    \n",
    "                common_keywords = list(set(keywords_tech1) & set(keywords_tech2))\n",
    "                keywords_t1 = list(set(keywords_tech1))\n",
    "                keywords_t2 = list(set(keywords_tech2))\n",
    "    \n",
    "                df_tech_1_2_common_keywords = dfinfos.loc[dfinfos.keyword.isin(common_keywords)]\n",
    "    \n",
    "                if len(common_keywords)==0:\n",
    "                    index_keywords = 0\n",
    "                else:\n",
    "                    list_comp_sum_keywords_t1_t2 = list(map(lambda x: importance_keywords_t1_t2(x, df_tech_1_2_common_keywords,tech1,tech2), common_keywords))\n",
    "                    index_keywords = sum(list_comp_sum_keywords_t1_t2)\n",
    "    \n",
    "            listindices.append(index_keywords)\n",
    "    \n",
    "    end = time.time()\n",
    "    year = dfinfos.year.tolist()[0]\n",
    "    month = dfinfos.month.tolist()[0]\n",
    "    print('Indices-keywords of proximity for '+str(year)+' in '+str(month)+' were computed in '+str(round(end-start,2))+' seconds.')\n",
    "\n",
    "    return listindices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43677e",
   "metadata": {},
   "source": [
    "The function below computes the index of proximity regarding colaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_colab(dfinfos,list_concepts):\n",
    "    \n",
    "    start = time.time()\n",
    "    listindices= []\n",
    "    \n",
    "    for tech1 in list_concepts:\n",
    "        for tech2 in list_concepts:\n",
    "\n",
    "            # variables for my computations\n",
    "    \n",
    "            dftech1 =dfinfos.loc[dfinfos[tech1]!=0]\n",
    "            dftech2 =dfinfos.loc[dfinfos[tech2]!=0]\n",
    "    \n",
    "            if len(dftech2) == 0 or len(dftech1)==0:\n",
    "               # this means we can not compute anything because we have no data\n",
    "                # we decide to let it blank.\n",
    "                index_colab_notincrem=np.nan\n",
    "                index_colab_increm = np.nan\n",
    "            else:\n",
    "    \n",
    "            # colab\n",
    "    \n",
    "                authors_tech1 = dftech1.author.tolist()\n",
    "                authors_tech2 = dftech2.author.tolist()\n",
    "    \n",
    "                common_authors = list(set(authors_tech1) & set(authors_tech2))\n",
    "                authors_t1 = list(set(authors_tech1))\n",
    "                authors_t2 = list(set(authors_tech2))\n",
    "                if len(common_authors)==0:\n",
    "                    index_colab_notincrem=0\n",
    "                    index_colab_increm = 0\n",
    "                else:\n",
    "                    unionauthors = list(set(authors_tech1) | set(authors_tech2))\n",
    "    \n",
    "                    df_tech_1_2_union_authors = dfinfos.loc[dfinfos.author.isin(unionauthors)]\n",
    "    \n",
    "                    list_comp_sum_authors_t1_t2_notincrem = list(map(lambda x: importance_author_t1_t2(x, df_tech_1_2_union_authors,tech1,tech2,'nonincremental'), common_authors))\n",
    "                    index_colab_notincrem = sum(list_comp_sum_authors_t1_t2_notincrem)\n",
    "        \n",
    "                    list_comp_sum_authors_t1_t2_increm = list(map(lambda x: importance_author_t1_t2(x, df_tech_1_2_union_authors,tech1,tech2,'incremental'), common_authors))\n",
    "                    index_colab_increm = sum(list_comp_sum_authors_t1_t2_increm)\n",
    "    \n",
    "       \n",
    "            listindices.append([index_colab_notincrem,index_colab_increm])\n",
    "        \n",
    "    end = time.time()\n",
    "    year = dfinfos.year.tolist()[0]\n",
    "    month = dfinfos.month.tolist()[0]\n",
    "    print('Indices-colab of proximity for '+str(year)+' in '+str(month)+' were computed in '+str(round(end-start,2))+' seconds.')\n",
    "\n",
    "    return listindices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a024f07",
   "metadata": {},
   "source": [
    "The function below computes the index of proximity regarding citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_cit(dfinfos,listconcepts,df_ref_help):\n",
    "    \n",
    "    start = time.time()\n",
    "    listindices= []\n",
    "    \n",
    "    for tech1 in list_concepts:\n",
    "        for tech2 in list_concepts:\n",
    "\n",
    "            # variables for my computations\n",
    "    \n",
    "            dftech1 =dfinfos.loc[dfinfos[tech1]!=0]\n",
    "            dftech2 =dfinfos.loc[dfinfos[tech2]!=0]\n",
    "\n",
    "            if len(dftech2) == 0 or len(dftech1)==0:\n",
    "            # this means we can not compute anything because we have no data we decide to let it blank.\n",
    "                index_cit_2_1=np.nan\n",
    "                index_cit_1_2 = np.nan\n",
    "            else:\n",
    "                papers_cit_t1 = list(set(dftech1.paper.tolist()))\n",
    "                papers_cit_t2 = list(set(dftech2.paper.tolist()))\n",
    " \n",
    "                df_t1_ref_t2 = dftech1.loc[dftech1['referenced_works'].isin(papers_cit_t2)]\n",
    "                papers_cit_t1_t2 = list(set(df_t1_ref_t2.paper.tolist()))\n",
    "                \n",
    "                df_t2_ref_t1 = dftech2.loc[dftech2['referenced_works'].isin(papers_cit_t1)]\n",
    "                papers_cit_t2_t1 = list(set(df_t2_ref_t1.paper.tolist()))\n",
    "\n",
    "    \n",
    "                if len(papers_cit_t1_t2)==0:\n",
    "                    index_cit_1_2 = 0\n",
    "                if len(papers_cit_t2_t1)==0:\n",
    "                    index_cit_2_1 = 0\n",
    "                if len(papers_cit_t1_t2)!=0:\n",
    "                    list_comp_sum_cit_t1_t2 = list(map(lambda x: importance_cit_tech(x, dftech1,tech1,tech2,df_ref_help), papers_cit_t1_t2))\n",
    "                    index_cit_1_2 = sum(list_comp_sum_cit_t1_t2)\n",
    "                if len(papers_cit_t2_t1)!=0:\n",
    "                    list_comp_sum_cit_t2_t1 = list(map(lambda x: importance_cit_tech(x, dftech2,tech2,tech1,df_ref_help), papers_cit_t2_t1))\n",
    "                    index_cit_2_1 = sum(list_comp_sum_cit_t2_t1)\n",
    "    \n",
    "            listindices.append([index_cit_1_2,index_cit_2_1])\n",
    "        \n",
    "    end = time.time()\n",
    "    year = dfinfos.year.tolist()[0]\n",
    "    month = dfinfos.month.tolist()[0]\n",
    "    \n",
    "    print('Indices-cit of proximity for '+str(year)+' in '+str(month)+' were computed in '+str(round(end-start,2))+' seconds.')\n",
    "    \n",
    "    return listindices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0414402",
   "metadata": {},
   "source": [
    "# **Part 3 -- Proximity indices based on keywords**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41344c",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "3.1. Computation the indices\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52569b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_concepts= ['Authentication protocole','Biometrics','Blockchain','Digital rights management'\n",
    ",'Digital signature','Distributed algorithm','Electronic voting','Functional encryption',\n",
    "'Hardware acceleration','Hardware security module','Hash function','Homomorphic encryption','Identity management',\n",
    "'Key management','Link encryption','Post-quantum cryptography','Public-key cryptography','Quantum key distribution',\n",
    "'Quantum cryptography','Random number generation','Symmetric-key algorithm','Threshold cryptosystem',\n",
    "'Trusted Computing','Tunneling protocol','Zero-knowlegde proof']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f38d72",
   "metadata": {},
   "source": [
    "I select only the information I need, to reduce my database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key = df_final.drop(['referenced_works','title','publication_date','abstract','author','yearly_H_index_notincremental','yearly_H_index_incremental','monthly_H_index_incremental','monthly_H_index_notincremental'], axis=1).copy()\n",
    "df_key = df_key.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772b780",
   "metadata": {},
   "source": [
    "With the function below, I compute a dataframe of indices based on keywords for all months and all combinations of technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd572c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_key = df_key.groupby(['year','month']).apply(lambda x: create_indices_keywords(x,list_concepts)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9a28d",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "3.2. Restructuration of the dataframe\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc79fc4",
   "metadata": {},
   "source": [
    "We now aim to turn the dataframe 'indices_key' into a dataframe as we want it (with columns and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d51b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key.rename(columns={0: \"index_keyword\"}, inplace=True)\n",
    "indices_key.reset_index(inplace=True, level=['month'])\n",
    "indices_key.reset_index(inplace=True, level=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331c5e1",
   "metadata": {},
   "source": [
    "We need to create lists for the concepts for the columns we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combination = []\n",
    "for tech1 in list_concepts:\n",
    "    for tech2 in list_concepts:\n",
    "        list_combination.append(tech1)\n",
    "        list_combination.append(tech2)\n",
    "        \n",
    "end = len(list_combination)-1\n",
    "first_listconcept = list_combination[:end:2]\n",
    "second_listconcept = list_combination[1:(end+1):2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0b587",
   "metadata": {},
   "source": [
    "We now explode the column 'indices' and add the columns about the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key= indices_key.explode('index_keyword')\n",
    "indices_key['concept1']=12*21*first_listconcept\n",
    "indices_key['concept2']=12*21*second_listconcept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b1130",
   "metadata": {},
   "source": [
    "We are done with our last modification. We just verify and save the dataframe for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key.to_pickle('data_indices/indices_key_notnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870caff2",
   "metadata": {},
   "source": [
    "# **Part 4 -- Proximity indices based on collaboration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4cc08",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "4.1. Computation the indices\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cec89",
   "metadata": {},
   "source": [
    "I select only the information I need, to reduce my database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colab = df_final.drop(['referenced_works','title','publication_date','abstract','keyword','cosine_similarity'], axis=1).copy()\n",
    "df_colab = df_colab.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800446",
   "metadata": {},
   "source": [
    "With the function below, I compute a dataframe of indices based on collaboration for all months and all combinations of technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6901ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_colab = df_colab.groupby(['year','month']).apply(lambda x: create_indices_colab(x,list_concepts)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322c3a8",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "4.2. Restructuration of the dataframe\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5f6f2",
   "metadata": {},
   "source": [
    "We now aim to turn the dataframe 'indices_colab' into a dataframe as we want it (with columns and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab.rename(columns={0: \"indices\"}, inplace=True)\n",
    "indices_colab.reset_index(inplace=True, level=['month'])\n",
    "indices_colab.reset_index(inplace=True, level=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f593f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645af809",
   "metadata": {},
   "source": [
    "We need to create lists for the concepts for the columns we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c40e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combination = []\n",
    "for tech1 in list_concepts:\n",
    "    for tech2 in list_concepts:\n",
    "        list_combination.append(tech1)\n",
    "        list_combination.append(tech2)\n",
    "        \n",
    "end = len(list_combination)-1\n",
    "first_listconcept = list_combination[:end:2]\n",
    "second_listconcept = list_combination[1:(end+1):2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef582c",
   "metadata": {},
   "source": [
    "We now explode the column 'indices' and add the columns about the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01864d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab= indices_colab.explode('indices')\n",
    "indices_colab['concept1']= 12*21*first_listconcept\n",
    "indices_colab['concept2']= 12*21*second_listconcept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d67b7",
   "metadata": {},
   "source": [
    "We take all the indices, which is a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3af3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "allindices = list(indices_colab.indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = len(allindices)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d235b98",
   "metadata": {},
   "source": [
    "We do a list of the indices based on not incremental monthly h-indices and the ones based on incremental monthly h-indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1=[x[0] for x in allindices]\n",
    "indices2= [x[1] for x in allindices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b62617",
   "metadata": {},
   "source": [
    "We update our dataframe adding the wanted information and eliminating the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe13cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab=indices_colab.drop('indices',axis=1)\n",
    "indices_colab['index_colab_notincrem']=indices1\n",
    "indices_colab['index_colab_increm']=indices2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e19c58",
   "metadata": {},
   "source": [
    "We are done with our last modification. We just verify and save the dataframe for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64785a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab.to_pickle('data_indices/indices_colab_notnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c7c65",
   "metadata": {},
   "source": [
    "# **Part 5 -- Proximity indices based on citations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559265f",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "5.1. Computation the indices\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e50088",
   "metadata": {},
   "source": [
    "I select only the information I need, to reduce my database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efff061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cit = df_final.drop(['keyword','author','cosine_similarity','title','publication_date','abstract','yearly_H_index_notincremental','yearly_H_index_incremental','monthly_H_index_incremental','monthly_H_index_notincremental'], axis=1).copy()\n",
    "df_cit = df_cit.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacab85f",
   "metadata": {},
   "source": [
    "I select only the information I need from my auxiliary dataframe for the referenced works, to reduce my database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0768f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_help = df_auxiliary_ref.drop(['year','month','author','title','publication_date','abstract'], axis=1).copy()\n",
    "df_ref_help = df_ref_help.drop_duplicates()\n",
    "df_ref_help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce7624",
   "metadata": {},
   "source": [
    "With the function below, I compute a dataframe of indices based on citations for all months and all combinations of technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b798f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bb303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_cit = df_cit.groupby(['year','month']).apply(lambda x: create_indices_cit(x,list_concepts,df_ref_help)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940f6eb",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "5.2. Restructuration of the dataframe\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit.to_pickle('data_indices/raw_indices_cit_notnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7db48",
   "metadata": {},
   "source": [
    "We now aim to turn the dataframe 'indices_cit' into a dataframe as we want it (with columns and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit.rename(columns={0: \"indices\"}, inplace=True)\n",
    "indices_cit.reset_index(inplace=True, level=['month'])\n",
    "indices_cit.reset_index(inplace=True, level=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a278da",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754feccc",
   "metadata": {},
   "source": [
    "We need to create lists for the concepts for the columns we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combination = []\n",
    "for tech1 in list_concepts:\n",
    "    for tech2 in list_concepts:\n",
    "        list_combination.append(tech1)\n",
    "        list_combination.append(tech2)\n",
    "        \n",
    "end = len(list_combination)-1\n",
    "first_listconcept = list_combination[:end:2]\n",
    "second_listconcept = list_combination[1:(end+1):2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abae317",
   "metadata": {},
   "source": [
    "We now explode the column 'indices' and add the columns about the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit= indices_cit.explode('indices')\n",
    "indices_cit['concept1']=12*21*first_listconcept\n",
    "indices_cit['concept2']=12*21*second_listconcept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea073dca",
   "metadata": {},
   "source": [
    "We take all the indices, which is a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "allindices = list(indices_cit.indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957f626",
   "metadata": {},
   "source": [
    "We do a list of the indices based on not incremental monthly h-indices and the ones based on incremental monthly h-indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1=[x[0] for x in allindices]\n",
    "indices2= [x[1] for x in allindices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aad603",
   "metadata": {},
   "source": [
    "We update our dataframe adding the wanted information and eliminating the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit=indices_cit.drop('indices',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ce4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit['index_cit_t1_t2']=indices1\n",
    "indices_cit['index_cit_t2_t1']=indices2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1618c2",
   "metadata": {},
   "source": [
    "We are done with our last modification. We just verify and save the dataframe for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0d73d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit.to_pickle('data_indices/indices_cit_notnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6872efc",
   "metadata": {},
   "source": [
    "# **Part 6 -- Merging the data altogether**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f54f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_indices_colab= open('data_indices/indices_colab_notnormalized','rb')\n",
    "indices_colab = pickle.load(infile_indices_colab)\n",
    "infile_indices_colab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372926cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_indices_key = open('data_indices/indices_key_notnormalized','rb')\n",
    "indices_key = pickle.load(infile_indices_key)\n",
    "infile_indices_key.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_indices_cit = open('data_indices/indices_cit_notnormalized','rb')\n",
    "indices_cit = pickle.load(infile_indices_cit)\n",
    "infile_indices_cit.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0834975",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026df9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e009a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfintermed = pd.merge(indices_cit,indices_key, on=['year','month','concept1','concept2'], how='right')\n",
    "dfintermed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices = pd.merge(dfintermed,indices_colab, on=['year','month','concept1','concept2'], how='right')\n",
    "dfindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466594e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices.to_pickle('data_indices/dfindices_notnormalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a0f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
