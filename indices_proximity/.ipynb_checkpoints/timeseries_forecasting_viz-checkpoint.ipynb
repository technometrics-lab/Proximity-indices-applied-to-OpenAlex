{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf53cba9",
   "metadata": {},
   "source": [
    "# *Forecasting of time series*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d95fb",
   "metadata": {},
   "source": [
    "__Introduction__\n",
    "\n",
    "In this notebook, we aim to forecast the time series we have using different methods.\n",
    "We will first forecast each time serie based on its own data. Then using the clustering we obtained with the clustering algorithms, we will forecast the time series by groups defined by their type.\n",
    "Finally, we will use the very big set of time series from darts (approximately 48'000 time series), to fit our forecasting algorithms and to predict the behavior of the time series. Naturally, we will also do a grid search to choose for the best hyperparameters of each model.\n",
    "Ultimately, the goal will be to predict the compare the different forecasting methods and to choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17a80c",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES\n",
    "\n",
    "Before we get started, let's import all necessary libraries for performing our time series classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "9577e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils import statistics\n",
    "from darts.utils.losses import SmapeLoss\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape\n",
    "from darts.utils.utils import SeasonalityMode, TrendMode, ModelMode\n",
    "from darts.models import *\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.models import TCNModel\n",
    "from darts.utils.likelihood_models import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6aef2",
   "metadata": {},
   "source": [
    "Let us now import the files we will need to forecast our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "b12992ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_dfindices = open('data_indices/dfindices_timeseries','rb')\n",
    "dfindices = pickle.load(infile_dfindices)\n",
    "infile_dfindices.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "ef859f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_dfcluster = open('data_indices/df_tm_tocluster','rb')\n",
    "dfcluster = pickle.load(infile_dfcluster)\n",
    "infile_dfcluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "42bd254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_df_interpolation = open('data_indices/df_interpolation_ratio','rb')\n",
    "df_interpolation = pickle.load(infile_df_interpolation)\n",
    "infile_df_interpolation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15517e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "a02e48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_exp_smooth(d,alpha=0.4):  \n",
    "    mylist= d  # Transform the input into a numpy array \n",
    "    cols = len(mylist)  # Historical period length  \n",
    "\n",
    "    f = list(np.full(cols,np.nan))  # Forecast array  \n",
    "    f[0] = mylist[0]  # initialization of first forecast  \n",
    "      # Create all the t+1 forecasts until end of historical period  \n",
    "    for t in range(1,cols):  \n",
    "        f[t] = alpha*mylist[t-1]+(1-alpha)*f[t-1]  \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c660eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5e8222",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "1. Filtering the dataframe by interpolation ratio\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd4cc8",
   "metadata": {},
   "source": [
    "As done before, we aim to forecast and consider only the time series that are meaningful, which means the time series that have a low interpolation ratio. This is done to avoid useless noise in our analyses and forecasting. For this reason, we keep only the time series with less than 50% of interpolation ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "087b2847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>index</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Biometrics</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Digital rights management</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Digital signature</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Symmetric-key algorithm</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Threshold cryptosystem</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Trusted Computing</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3125 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      concept1                   concept2            index  \\\n",
       "0     Authentication protocole   Authentication protocole  index_cit_t1_t2   \n",
       "1     Authentication protocole                 Biometrics  index_cit_t1_t2   \n",
       "2     Authentication protocole                 Blockchain  index_cit_t1_t2   \n",
       "3     Authentication protocole  Digital rights management  index_cit_t1_t2   \n",
       "4     Authentication protocole          Digital signature  index_cit_t1_t2   \n",
       "...                        ...                        ...              ...   \n",
       "3120      Zero-knowlegde proof    Symmetric-key algorithm    index_keyword   \n",
       "3121      Zero-knowlegde proof     Threshold cryptosystem    index_keyword   \n",
       "3122      Zero-knowlegde proof          Trusted Computing    index_keyword   \n",
       "3123      Zero-knowlegde proof         Tunneling protocol    index_keyword   \n",
       "3124      Zero-knowlegde proof       Zero-knowlegde proof    index_keyword   \n",
       "\n",
       "      cluster  \n",
       "0         0.0  \n",
       "1         5.0  \n",
       "2         5.0  \n",
       "3         5.0  \n",
       "4         5.0  \n",
       "...       ...  \n",
       "3120      2.0  \n",
       "3121      NaN  \n",
       "3122      4.0  \n",
       "3123      NaN  \n",
       "3124      1.0  \n",
       "\n",
       "[3125 rows x 4 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "bcab2b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>date</th>\n",
       "      <th>index_cit_t1_t2</th>\n",
       "      <th>index_cit_t2_t1</th>\n",
       "      <th>index_colab_increm</th>\n",
       "      <th>index_colab_notincrem</th>\n",
       "      <th>index_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>1-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>3-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>4-01-2</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>13.808524</td>\n",
       "      <td>13.77767</td>\n",
       "      <td>7.22864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>5-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>3.355919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>8-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.317264</td>\n",
       "      <td>28.72956</td>\n",
       "      <td>8.031585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>9-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>8.877465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>10-01-22</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>48.748713</td>\n",
       "      <td>44.762499</td>\n",
       "      <td>14.777641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>11-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.238745</td>\n",
       "      <td>16.901309</td>\n",
       "      <td>6.616964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>12-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.479787</td>\n",
       "      <td>32.221229</td>\n",
       "      <td>10.601732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     concept1                  concept2      date  \\\n",
       "0    Authentication protocole  Authentication protocole    1-01-2   \n",
       "0    Authentication protocole  Authentication protocole    2-01-2   \n",
       "0    Authentication protocole  Authentication protocole    3-01-2   \n",
       "0    Authentication protocole  Authentication protocole    4-01-2   \n",
       "0    Authentication protocole  Authentication protocole    5-01-2   \n",
       "..                        ...                       ...       ...   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   8-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   9-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  10-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  11-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  12-01-22   \n",
       "\n",
       "    index_cit_t1_t2 index_cit_t2_t1 index_colab_increm index_colab_notincrem  \\\n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0          0.836174        0.836174          13.808524              13.77767   \n",
       "0               0.0             0.0           6.284266              6.284266   \n",
       "..              ...             ...                ...                   ...   \n",
       "624             0.0             0.0          54.317264              28.72956   \n",
       "624             0.0             0.0          19.811067             19.811067   \n",
       "624        6.706388        6.706388          48.748713             44.762499   \n",
       "624             0.0             0.0          24.238745             16.901309   \n",
       "624             0.0             0.0          85.479787             32.221229   \n",
       "\n",
       "    index_keyword  \n",
       "0        1.720479  \n",
       "0        1.720479  \n",
       "0        1.720479  \n",
       "0         7.22864  \n",
       "0        3.355919  \n",
       "..            ...  \n",
       "624      8.031585  \n",
       "624      8.877465  \n",
       "624     14.777641  \n",
       "624      6.616964  \n",
       "624     10.601732  \n",
       "\n",
       "[157500 rows x 8 columns]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f676515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>interpolation_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Threshold cryptosystem</td>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Threshold cryptosystem</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Link encryption</td>\n",
       "      <td>0.924603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Digital signature</td>\n",
       "      <td>Hash function</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Public-key cryptography</td>\n",
       "      <td>Hash function</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Public-key cryptography</td>\n",
       "      <td>Public-key cryptography</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Hash function</td>\n",
       "      <td>Public-key cryptography</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Hash function</td>\n",
       "      <td>Hash function</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>625 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    concept1                 concept2  interpolation_ratio\n",
       "548   Threshold cryptosystem       Tunneling protocol             0.944444\n",
       "596       Tunneling protocol   Threshold cryptosystem             0.944444\n",
       "577       Tunneling protocol               Blockchain             0.928571\n",
       "73                Blockchain       Tunneling protocol             0.928571\n",
       "589       Tunneling protocol          Link encryption             0.924603\n",
       "..                       ...                      ...                  ...\n",
       "110        Digital signature            Hash function             0.007937\n",
       "410  Public-key cryptography            Hash function             0.000000\n",
       "416  Public-key cryptography  Public-key cryptography             0.000000\n",
       "266            Hash function  Public-key cryptography             0.000000\n",
       "260            Hash function            Hash function             0.000000\n",
       "\n",
       "[625 rows x 3 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "3ef595b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>interpolation_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Threshold cryptosystem</td>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Threshold cryptosystem</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Tunneling protocol</td>\n",
       "      <td>Link encryption</td>\n",
       "      <td>0.924603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Hardware security module</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>0.531746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Random number generation</td>\n",
       "      <td>0.503968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Random number generation</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>0.503968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Blockchain</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     concept1                  concept2  interpolation_ratio\n",
       "548    Threshold cryptosystem        Tunneling protocol             0.944444\n",
       "596        Tunneling protocol    Threshold cryptosystem             0.944444\n",
       "577        Tunneling protocol                Blockchain             0.928571\n",
       "73                 Blockchain        Tunneling protocol             0.928571\n",
       "589        Tunneling protocol           Link encryption             0.924603\n",
       "..                        ...                       ...                  ...\n",
       "227  Hardware security module                Blockchain             0.531746\n",
       "69                 Blockchain  Random number generation             0.503968\n",
       "477  Random number generation                Blockchain             0.503968\n",
       "602      Zero-knowlegde proof                Blockchain             0.500000\n",
       "74                 Blockchain      Zero-knowlegde proof             0.500000\n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolation.loc[df_interpolation['interpolation_ratio']>=0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "1cf2de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_concepts= ['Authentication protocole','Biometrics','Blockchain','Digital rights management'\n",
    ",'Digital signature','Distributed algorithm','Electronic voting','Functional encryption',\n",
    "'Hardware acceleration','Hardware security module','Hash function','Homomorphic encryption','Identity management',\n",
    "'Key management','Link encryption','Post-quantum cryptography','Public-key cryptography','Quantum key distribution',\n",
    "'Quantum cryptography','Random number generation','Symmetric-key algorithm','Threshold cryptosystem',\n",
    "'Trusted Computing','Tunneling protocol','Zero-knowlegde proof']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "22592d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indices=['index_cit_t1_t2','index_cit_t2_t1','index_colab_increm','index_colab_notincrem','index_keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a50456cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_interpolationratio=[]\n",
    "\n",
    "numbertm_highextrapolation = 0\n",
    "\n",
    "for concept1 in list_concepts:\n",
    "    dfinter1=df_interpolation.loc[df_interpolation['concept1']==concept1]\n",
    "    for concept2 in list_concepts:\n",
    "        dfinter2=dfinter1.loc[dfinter1['concept2']==concept2]\n",
    "        interpolation_score = dfinter2.interpolation_ratio.tolist()[0]\n",
    "        list_interpolationratio=list_interpolationratio+252*[interpolation_score]\n",
    "        if interpolation_score>=0.5:\n",
    "            numbertm_highextrapolation=numbertm_highextrapolation+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e89fd9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 155 pairs of technologies with a high interpolation rate\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(numbertm_highextrapolation)+' pairs of technologies with a high interpolation rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "50d6aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices['interpolation_score']=list_interpolationratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "11c7598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices=dfindices.drop(dfindices[dfindices['interpolation_score'] >=0.5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "65fc6360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>date</th>\n",
       "      <th>index_cit_t1_t2</th>\n",
       "      <th>index_cit_t2_t1</th>\n",
       "      <th>index_colab_increm</th>\n",
       "      <th>index_colab_notincrem</th>\n",
       "      <th>index_keyword</th>\n",
       "      <th>interpolation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>1-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>3-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>4-01-2</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>13.808524</td>\n",
       "      <td>13.77767</td>\n",
       "      <td>7.22864</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>5-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>3.355919</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>8-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.317264</td>\n",
       "      <td>28.72956</td>\n",
       "      <td>8.031585</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>9-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>8.877465</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>10-01-22</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>48.748713</td>\n",
       "      <td>44.762499</td>\n",
       "      <td>14.777641</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>11-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.238745</td>\n",
       "      <td>16.901309</td>\n",
       "      <td>6.616964</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>12-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.479787</td>\n",
       "      <td>32.221229</td>\n",
       "      <td>10.601732</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118440 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     concept1                  concept2      date  \\\n",
       "0    Authentication protocole  Authentication protocole    1-01-2   \n",
       "0    Authentication protocole  Authentication protocole    2-01-2   \n",
       "0    Authentication protocole  Authentication protocole    3-01-2   \n",
       "0    Authentication protocole  Authentication protocole    4-01-2   \n",
       "0    Authentication protocole  Authentication protocole    5-01-2   \n",
       "..                        ...                       ...       ...   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   8-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   9-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  10-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  11-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  12-01-22   \n",
       "\n",
       "    index_cit_t1_t2 index_cit_t2_t1 index_colab_increm index_colab_notincrem  \\\n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0          0.836174        0.836174          13.808524              13.77767   \n",
       "0               0.0             0.0           6.284266              6.284266   \n",
       "..              ...             ...                ...                   ...   \n",
       "624             0.0             0.0          54.317264              28.72956   \n",
       "624             0.0             0.0          19.811067             19.811067   \n",
       "624        6.706388        6.706388          48.748713             44.762499   \n",
       "624             0.0             0.0          24.238745             16.901309   \n",
       "624             0.0             0.0          85.479787             32.221229   \n",
       "\n",
       "    index_keyword  interpolation_score  \n",
       "0        1.720479             0.019841  \n",
       "0        1.720479             0.019841  \n",
       "0        1.720479             0.019841  \n",
       "0         7.22864             0.019841  \n",
       "0        3.355919             0.019841  \n",
       "..            ...                  ...  \n",
       "624      8.031585             0.095238  \n",
       "624      8.877465             0.095238  \n",
       "624     14.777641             0.095238  \n",
       "624      6.616964             0.095238  \n",
       "624     10.601732             0.095238  \n",
       "\n",
       "[118440 rows x 9 columns]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfindices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d7ed3",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "2. Transforming the datatype of date to obtain darts time series\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670799e5",
   "metadata": {},
   "source": [
    "We now must transform the datatype of the column 'date' to get it acceptable for darts time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "619382fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices['time'] = pd.Series(['12:00' for x in range(len(dfindices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "3eb94e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = dfindices.date.str.cat(dfindices.time,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "8d5ff919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices['date_and_time']=pd.to_datetime(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "8df9c55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>date</th>\n",
       "      <th>index_cit_t1_t2</th>\n",
       "      <th>index_cit_t2_t1</th>\n",
       "      <th>index_colab_increm</th>\n",
       "      <th>index_colab_notincrem</th>\n",
       "      <th>index_keyword</th>\n",
       "      <th>interpolation_score</th>\n",
       "      <th>time</th>\n",
       "      <th>date_and_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>1-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2002-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2002-02-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>3-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2002-03-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>4-01-2</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>13.808524</td>\n",
       "      <td>13.77767</td>\n",
       "      <td>7.22864</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2002-04-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>5-01-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>3.355919</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2002-05-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>8-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.317264</td>\n",
       "      <td>28.72956</td>\n",
       "      <td>8.031585</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2022-08-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>9-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>8.877465</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2022-09-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>10-01-22</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>48.748713</td>\n",
       "      <td>44.762499</td>\n",
       "      <td>14.777641</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2022-10-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>11-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.238745</td>\n",
       "      <td>16.901309</td>\n",
       "      <td>6.616964</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2022-11-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>12-01-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.479787</td>\n",
       "      <td>32.221229</td>\n",
       "      <td>10.601732</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>12:00</td>\n",
       "      <td>2022-12-01 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118440 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     concept1                  concept2      date  \\\n",
       "0    Authentication protocole  Authentication protocole    1-01-2   \n",
       "0    Authentication protocole  Authentication protocole    2-01-2   \n",
       "0    Authentication protocole  Authentication protocole    3-01-2   \n",
       "0    Authentication protocole  Authentication protocole    4-01-2   \n",
       "0    Authentication protocole  Authentication protocole    5-01-2   \n",
       "..                        ...                       ...       ...   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   8-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof   9-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  10-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  11-01-22   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof  12-01-22   \n",
       "\n",
       "    index_cit_t1_t2 index_cit_t2_t1 index_colab_increm index_colab_notincrem  \\\n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0          0.836174        0.836174          13.808524              13.77767   \n",
       "0               0.0             0.0           6.284266              6.284266   \n",
       "..              ...             ...                ...                   ...   \n",
       "624             0.0             0.0          54.317264              28.72956   \n",
       "624             0.0             0.0          19.811067             19.811067   \n",
       "624        6.706388        6.706388          48.748713             44.762499   \n",
       "624             0.0             0.0          24.238745             16.901309   \n",
       "624             0.0             0.0          85.479787             32.221229   \n",
       "\n",
       "    index_keyword  interpolation_score   time       date_and_time  \n",
       "0        1.720479             0.019841  12:00 2002-01-01 12:00:00  \n",
       "0        1.720479             0.019841  12:00 2002-02-01 12:00:00  \n",
       "0        1.720479             0.019841  12:00 2002-03-01 12:00:00  \n",
       "0         7.22864             0.019841  12:00 2002-04-01 12:00:00  \n",
       "0        3.355919             0.019841  12:00 2002-05-01 12:00:00  \n",
       "..            ...                  ...    ...                 ...  \n",
       "624      8.031585             0.095238  12:00 2022-08-01 12:00:00  \n",
       "624      8.877465             0.095238  12:00 2022-09-01 12:00:00  \n",
       "624     14.777641             0.095238  12:00 2022-10-01 12:00:00  \n",
       "624      6.616964             0.095238  12:00 2022-11-01 12:00:00  \n",
       "624     10.601732             0.095238  12:00 2022-12-01 12:00:00  \n",
       "\n",
       "[118440 rows x 11 columns]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4e0f4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfindices = dfindices.drop(['date','time'], axis=1).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "8fce78c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>date_and_time</th>\n",
       "      <th>index_cit_t1_t2</th>\n",
       "      <th>index_cit_t2_t1</th>\n",
       "      <th>index_colab_increm</th>\n",
       "      <th>index_colab_notincrem</th>\n",
       "      <th>index_keyword</th>\n",
       "      <th>interpolation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2002-01-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2002-02-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2002-03-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>2.699103</td>\n",
       "      <td>1.720479</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2002-04-01 12:00:00</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>0.836174</td>\n",
       "      <td>13.808524</td>\n",
       "      <td>13.77767</td>\n",
       "      <td>7.22864</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>2002-05-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>6.284266</td>\n",
       "      <td>3.355919</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>2022-08-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.317264</td>\n",
       "      <td>28.72956</td>\n",
       "      <td>8.031585</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>2022-09-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>19.811067</td>\n",
       "      <td>8.877465</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>2022-10-01 12:00:00</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>6.706388</td>\n",
       "      <td>48.748713</td>\n",
       "      <td>44.762499</td>\n",
       "      <td>14.777641</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>2022-11-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.238745</td>\n",
       "      <td>16.901309</td>\n",
       "      <td>6.616964</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>2022-12-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.479787</td>\n",
       "      <td>32.221229</td>\n",
       "      <td>10.601732</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118440 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     concept1                  concept2       date_and_time  \\\n",
       "0    Authentication protocole  Authentication protocole 2002-01-01 12:00:00   \n",
       "0    Authentication protocole  Authentication protocole 2002-02-01 12:00:00   \n",
       "0    Authentication protocole  Authentication protocole 2002-03-01 12:00:00   \n",
       "0    Authentication protocole  Authentication protocole 2002-04-01 12:00:00   \n",
       "0    Authentication protocole  Authentication protocole 2002-05-01 12:00:00   \n",
       "..                        ...                       ...                 ...   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof 2022-08-01 12:00:00   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof 2022-09-01 12:00:00   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof 2022-10-01 12:00:00   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof 2022-11-01 12:00:00   \n",
       "624      Zero-knowlegde proof      Zero-knowlegde proof 2022-12-01 12:00:00   \n",
       "\n",
       "    index_cit_t1_t2 index_cit_t2_t1 index_colab_increm index_colab_notincrem  \\\n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0               0.0             0.0           2.699103              2.699103   \n",
       "0          0.836174        0.836174          13.808524              13.77767   \n",
       "0               0.0             0.0           6.284266              6.284266   \n",
       "..              ...             ...                ...                   ...   \n",
       "624             0.0             0.0          54.317264              28.72956   \n",
       "624             0.0             0.0          19.811067             19.811067   \n",
       "624        6.706388        6.706388          48.748713             44.762499   \n",
       "624             0.0             0.0          24.238745             16.901309   \n",
       "624             0.0             0.0          85.479787             32.221229   \n",
       "\n",
       "    index_keyword  interpolation_score  \n",
       "0        1.720479             0.019841  \n",
       "0        1.720479             0.019841  \n",
       "0        1.720479             0.019841  \n",
       "0         7.22864             0.019841  \n",
       "0        3.355919             0.019841  \n",
       "..            ...                  ...  \n",
       "624      8.031585             0.095238  \n",
       "624      8.877465             0.095238  \n",
       "624     14.777641             0.095238  \n",
       "624      6.616964             0.095238  \n",
       "624     10.601732             0.095238  \n",
       "\n",
       "[118440 rows x 9 columns]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "9e000e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept1                         object\n",
       "concept2                         object\n",
       "date_and_time            datetime64[ns]\n",
       "index_cit_t1_t2                  object\n",
       "index_cit_t2_t1                  object\n",
       "index_colab_increm               object\n",
       "index_colab_notincrem            object\n",
       "index_keyword                    object\n",
       "interpolation_score             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfindices.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6277c0",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "3. Creation of a dataframe of darts time series\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "5f6d8e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470db889d3134f29b5ff91fd1b6c511b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "listconcept1=[]\n",
    "listconcept2=[]\n",
    "listindextype=[]\n",
    "listdartstm=[]\n",
    "\n",
    "for concept1 in tqdm(list_concepts):\n",
    "    dfconcept1 = dfindices.loc[dfindices['concept1']==concept1]\n",
    "    for concept2 in list_concepts:\n",
    "        dfconcept2 = dfconcept1.loc[dfconcept1['concept2']==concept2].copy()\n",
    "        subdf=dfconcept2.set_index('date_and_time')\n",
    "        if len(subdf)!=0:\n",
    "            for index in list_indices:\n",
    "                dftotimeseries=subdf[index]\n",
    "                firstimeseries = TimeSeries.from_series(dftotimeseries,freq='MS')\n",
    "                mytimeseries=statistics.remove_seasonality(firstimeseries,freq=12,model=ModelMode.ADDITIVE,method='STL')\n",
    "                #print(mytimeseries)\n",
    "                myvalues = list(TimeSeries.pd_series(mytimeseries))                \n",
    "                \n",
    "                smoothed_tm = simple_exp_smooth(myvalues,alpha=0.1)\n",
    "                mytm = pd.Series(smoothed_tm)\n",
    "                mytm.index = TimeSeries.pd_series(mytimeseries).index\n",
    "                \n",
    "                darts_tm = TimeSeries.from_series(mytm)\n",
    "                \n",
    "                #print(darts_tm)\n",
    "                listconcept1.append(concept1)\n",
    "                listconcept2.append(concept2)\n",
    "                listindextype.append(index)\n",
    "                listdartstm.append(darts_tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "2118692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm = pd.DataFrame({'concept1':listconcept1,'concept2':listconcept2,'indextype':listindextype,'Time serie':listdartstm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "180ecd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept1</th>\n",
       "      <th>concept2</th>\n",
       "      <th>indextype</th>\n",
       "      <th>Time serie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_cit_t2_t1</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_colab_increm</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_colab_notincrem</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>Authentication protocole</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_cit_t1_t2</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_cit_t2_t1</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_colab_increm</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_colab_notincrem</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>Zero-knowlegde proof</td>\n",
       "      <td>index_keyword</td>\n",
       "      <td>(((&lt;TimeSeries (DataArray) (date_and_time: 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      concept1                  concept2  \\\n",
       "0     Authentication protocole  Authentication protocole   \n",
       "1     Authentication protocole  Authentication protocole   \n",
       "2     Authentication protocole  Authentication protocole   \n",
       "3     Authentication protocole  Authentication protocole   \n",
       "4     Authentication protocole  Authentication protocole   \n",
       "...                        ...                       ...   \n",
       "2345      Zero-knowlegde proof      Zero-knowlegde proof   \n",
       "2346      Zero-knowlegde proof      Zero-knowlegde proof   \n",
       "2347      Zero-knowlegde proof      Zero-knowlegde proof   \n",
       "2348      Zero-knowlegde proof      Zero-knowlegde proof   \n",
       "2349      Zero-knowlegde proof      Zero-knowlegde proof   \n",
       "\n",
       "                  indextype                                         Time serie  \n",
       "0           index_cit_t1_t2  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "1           index_cit_t2_t1  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "2        index_colab_increm  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "3     index_colab_notincrem  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "4             index_keyword  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "...                     ...                                                ...  \n",
       "2345        index_cit_t1_t2  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "2346        index_cit_t2_t1  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "2347     index_colab_increm  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "2348  index_colab_notincrem  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "2349          index_keyword  (((<TimeSeries (DataArray) (date_and_time: 1, ...  \n",
       "\n",
       "[2350 rows x 4 columns]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "21776901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3125"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylistcluster = dfcluster.cluster.tolist()\n",
    "len(mylistcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "8f536536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclusters = [1+int(x) for x in mylistcluster if str(x)!='nan']\n",
    "len(myclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "7838d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm['cluster']=myclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec307a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9140c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing some time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,7):\n",
    "    mydfnow=dftm.loc[dftm['cluster']==i]\n",
    "\n",
    "    nowlist =mydfnow['Time serie'].tolist()[40:50]\n",
    "    for ele in nowlist:\n",
    "        ele.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(3, 2, figsize=(10, 10), dpi=100)\n",
    "\n",
    "for i, idx in enumerate([1, 20, 50, 103, 250, 300]):\n",
    "    axis = ax[i % 3, i % 2]\n",
    "    dftm.loc[idx]['Time serie'].plot(ax=axis,c=\"orangered\",lw=2,)\n",
    "    axis.legend(dftm.loc[idx]['Time serie'].components)\n",
    "    axis.set_title(\" \");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a74e2",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "4. Auxiliary function for forecasting\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e976b4",
   "metadata": {},
   "source": [
    "Defining auxiliary functions, I will use in my forecasting analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f84288",
   "metadata": {},
   "source": [
    "We define a handy function to tell us how good a bunch of forecasted series are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce80ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysmape(preds, test_series): \n",
    "    listsmapes = []\n",
    "    vector_local_variances = []\n",
    "    vector_all_errors = []\n",
    "    for j in range(len(preds)):\n",
    "        actual = [x[0] for x in TimeSeries.values(preds[j])]\n",
    "        forecast=[x[0] for x in TimeSeries.values(test_series[j])]\n",
    "        actual_new = []\n",
    "        forecast_new = []\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i]!=0 and forecast[i]!=0:\n",
    "                actual_new.append(actual[i])\n",
    "                forecast_new.append(forecast[i])\n",
    "        actual_new = np.array(actual_new)\n",
    "        forecast_new = np.array(forecast_new)\n",
    "        listsmapes.append(100*np.sum(2*np.abs(forecast_new - actual_new) / \\\n",
    "        (np.abs(actual_new) + np.abs(forecast_new)))/len(actual))\n",
    "        \n",
    "        vector_errors=[]\n",
    "        for i in range(len(actual)):\n",
    "            vector_errors.append(abs(actual[i]-forecast[i]))\n",
    "            \n",
    "        vector_local_variances.append(np.var(vector_errors))\n",
    "        \n",
    "        vector_all_errors = vector_all_errors+vector_errors\n",
    "    \n",
    "    var_allerrors = np.var(vector_all_errors)\n",
    "    var_mean_var = np.mean(vector_local_variances)\n",
    "    \n",
    "    return listsmapes,var_allerrors,var_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_var_local(forecaster,horizon,df,mean_allvar):\n",
    "    model_loaded = forecaster.load(\"my_model.pkl\")\n",
    "    \n",
    "    preds = list(TimeSeries.pd_series(model_loaded.predict(n=horizon)))\n",
    "\n",
    "    actualtm = list(TimeSeries.pd_series(df['Time serie'].tolist()[-1])) \n",
    "    \n",
    "    preds_minusvar =actualtm[:-horizon]\n",
    "    preds_plusvar = actualtm[:-horizon]\n",
    "    predictions = actualtm[:-horizon]+preds\n",
    "    \n",
    "    for j in range(horizon):\n",
    "        preds_minusvar.append(preds[j]-np.mean(mean_allvar)) \n",
    "        preds_plusvar.append(preds[j]+np.mean(mean_allvar)) \n",
    "    \n",
    "    x_cor =np.arange(0,len(actualtm))\n",
    "    plt.plot(x_cor,predictions,x_cor,preds_plusvar,x_cor,preds_minusvar)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c60ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_var_global(forecaster,horizon,df,mean_allvar):\n",
    "    model_loaded = forecaster.load(\"my_model.pkl\")\n",
    "    \n",
    "    preds = list(TimeSeries.pd_series(model_loaded.predict(n=horizon,series=df['Train tm'].tolist()[-1])))\n",
    "\n",
    "    actualtm = list(TimeSeries.pd_series(df['Time serie'].tolist()[-1])) \n",
    "    \n",
    "    preds_minusvar =actualtm[:-horizon]\n",
    "    preds_plusvar = actualtm[:-horizon]\n",
    "    predictions = actualtm[:-horizon]+preds\n",
    "    \n",
    "    for j in range(horizon):\n",
    "        preds_minusvar.append(preds[j]-np.mean(mean_allvar)) \n",
    "        preds_plusvar.append(preds[j]+np.mean(mean_allvar)) \n",
    "    \n",
    "    x_cor =np.arange(0,len(actualtm))\n",
    "    plt.plot(x_cor,predictions,x_cor,preds_plusvar,x_cor,preds_minusvar)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a44f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_splitting_window_local(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes = []\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =maximal_train_len//number_cv\n",
    "            startingtime = i*windowslength\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_local_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len-(number_cv-1)*(maximal_train_len//number_cv) # to get the remaining window\n",
    "            startingtime = i*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_local_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        allsmapes.append(smapes)\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "        \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "        \n",
    "        \n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "    \n",
    "    eval_kcross_forecasts(allsmapes,modelname)\n",
    "\n",
    "\n",
    "    \n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_expanding_window_local(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    \n",
    "    startingtime=0\n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes = []\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =(i+1)*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_local_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_local_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "        \n",
    "        allsmapes.append(smapes)\n",
    "        \n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "        \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "        \n",
    "\n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "        \n",
    "    eval_kcross_forecasts(allsmapes,modelname)\n",
    "\n",
    "\n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_splitting_window_global(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes=[]\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =maximal_train_len//number_cv\n",
    "            startingtime = i*windowslength\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len-(number_cv-1)*(maximal_train_len//number_cv) # to get the remaining window\n",
    "            startingtime = i*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "        \n",
    "        allsmapes.append(smapes)\n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "        \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "        \n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "    \n",
    "    eval_kcross_forecasts(allsmapes,modelname)\n",
    "    \n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd01342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_expanding_window_global(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    startingtime=0\n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes=[]\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =(i+1)*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        \n",
    "        allsmapes.append(smapes)\n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "        \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "\n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "    \n",
    "    eval_kcross_forecasts(allsmapes,modelname)\n",
    "\n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_train_test_tm(dftm,startingtime,windowslength,horizon,list_concepts):\n",
    "\n",
    "    listconcept1=[]\n",
    "    listconcept2=[]\n",
    "    listindextype=[]\n",
    "    \n",
    "    listdartstm=[]\n",
    "    listtraintm=[]\n",
    "    listtesttm=[]\n",
    "    listcluster=[]\n",
    "\n",
    "    for concept1 in list_concepts:\n",
    "        dfconcept1 = dftm.loc[dftm['concept1']==concept1]\n",
    "        for concept2 in list_concepts:\n",
    "            dfconcept2 = dfconcept1.loc[dfconcept1['concept2']==concept2]\n",
    "            for index in list_indices:\n",
    "                dfindex=dfconcept2.loc[dfconcept2['indextype']==index]\n",
    "                if len(dfindex)!=0:\n",
    "                    \n",
    "                    mycluster = dfindex.cluster.tolist()[0]\n",
    "                    mytimeseries=dfindex['Time serie'].tolist()[0]\n",
    "                    if startingtime==0:\n",
    "                        traintm=mytimeseries[:windowslength]\n",
    "                    else:\n",
    "                        traintm=mytimeseries[startingtime-1:startingtime+windowslength]\n",
    "                        \n",
    "                    testtm=mytimeseries[startingtime+windowslength:startingtime+windowslength+horizon]\n",
    "                                    \n",
    "                    listconcept1.append(concept1)\n",
    "                    listconcept2.append(concept2)\n",
    "                    listindextype.append(index)\n",
    "                    listdartstm.append(mytimeseries)\n",
    "                    listtraintm.append(traintm)\n",
    "                    listtesttm.append(testtm)\n",
    "                    listcluster.append(mycluster)\n",
    "                    \n",
    "    df = pd.DataFrame({'concept1':listconcept1,'concept2':listconcept2,'indextype':listindextype,\n",
    "                         'Time serie':listdartstm,'Train tm':listtraintm,'Test tm':listtesttm,'cluster':listcluster})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_kcross_forecasts(smapes,modelname):\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    themedians = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(smapes)):\n",
    "        themedians.append(np.median(smapes[i]))\n",
    "        #plt.hist(smapes[i], bins=100,label='K-cross set'+str(i+1))\n",
    "        labels = labels +['Trained on '+str((i+1)*(100//len(smapes)))+\\\n",
    "        ' % of the data, with a median smape of '+ str(round(np.median(smapes[i]),2))+' %.']\n",
    "        \n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"sMAPE\")\n",
    "    plt.title(\"The median sMAPE for \"+str(modelname)+\" : %.3f\" % np.median(themedians))\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    plt.hist(smapes,bins=100,histtype='barstacked',label=labels)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_local_model(train_series, test_series,horizon, model_cls, **kwargs):\n",
    "    preds = []\n",
    "\n",
    "    for series in tqdm(train_series):\n",
    "        model = model_cls(**kwargs)\n",
    "        model.fit(series)\n",
    "        pred = model.predict(n=horizon)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    smapes,var_allerrors,var_mean_var = mysmape(preds, test_series)\n",
    "    \n",
    "    model.save(\"my_model.pkl\")\n",
    "    \n",
    "    return smapes,var_allerrors,var_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c925c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_global_model(train_series, test_series,horizon, model_cls, **kwargs):\n",
    "    model = model_cls(**kwargs)\n",
    "\n",
    "    if model_cls==NBEATSModel:\n",
    "        model.fit(train_series, num_loader_workers=4, epochs=10)\n",
    "    else:\n",
    "        model.fit(train_series)\n",
    "    preds = model.predict(n=horizon, series=train_series)\n",
    "\n",
    "    smapes,var_allerrors,var_mean_var = mysmape(preds, test_series)\n",
    "    \n",
    "    model.save(\"my_model.pkl\")\n",
    "    \n",
    "    return smapes,var_allerrors,var_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models(method_to_elapsed_times, method_to_smapes):\n",
    "    shapes = [\"o\", \"s\", \"*\"]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:purple\"]\n",
    "    styles = list(product(shapes, colors))\n",
    "\n",
    "    plt.figure(figsize=(6, 6), dpi=100)\n",
    "\n",
    "    for i, method in enumerate(method_to_elapsed_times.keys()):\n",
    "\n",
    "        t = method_to_elapsed_times[method]\n",
    "        s = styles[i]\n",
    "        plt.semilogx(\n",
    "            [t],\n",
    "            [np.mean(method_to_smapes[method])],\n",
    "            s[0],\n",
    "            color=s[1],\n",
    "            label=method,\n",
    "            markersize=13,\n",
    "        )\n",
    "    plt.xlabel(\"elapsed time [s]\")\n",
    "    plt.ylabel(\"Average sMAPE over all series\")\n",
    "    plt.legend(bbox_to_anchor=(1.4, 1.0), frameon=True)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34953b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models_cluster(method_to_elapsed_times, method_to_smapes):\n",
    "    shapes = [\"o\", \"s\", \"*\"]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:purple\"]\n",
    "    styles = list(product(shapes, colors))\n",
    "\n",
    "    plt.figure(figsize=(6, 6), dpi=100)\n",
    "\n",
    "    for i, method in enumerate(method_to_elapsed_times.keys()):\n",
    "\n",
    "        t = method_to_elapsed_times[method]\n",
    "        s = styles[i]\n",
    "        plt.semilogx(\n",
    "            [t],\n",
    "            [np.mean(method_to_smapes[method])],\n",
    "            s[0],\n",
    "            color=s[1],\n",
    "            label=method,\n",
    "            markersize=13,\n",
    "        )\n",
    "    plt.xlabel(\"elapsed time [s]\")\n",
    "    plt.ylabel(\"Average sMAPE over all series\")\n",
    "    plt.legend(bbox_to_anchor=(1.4, 1.0), frameon=True)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0ed61",
   "metadata": {},
   "source": [
    "# **Part 1 -- Local forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1c4fa",
   "metadata": {},
   "source": [
    "Creating the dataframe I will use with an horizon of 3 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05887d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_forecasting(dftm,horizon,list_concepts,number_cv):\n",
    "    \n",
    "    # once I created my dataframe with this specific horizon I train and visualize the results of the forecasting algorithms\n",
    "    \n",
    "    naive1_time,naive1_smapes_mean,naive1_smapes_med,naive1_var_allerrors,naive1_mean_allvar =\\\n",
    "    cross_validation_expanding_window_local(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'NaiveSeasonal K=1',NaiveSeasonal, K=1)\n",
    "    \n",
    "    naive12_time,naive12_smapes_mean,naive12_smapes_med,naive12_var_allerrors,naive12_mean_allvar\\\n",
    "    =cross_validation_expanding_window_local(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'NaiveSeasonal K=12', NaiveSeasonal, K=12)\n",
    "    \n",
    "    ets_time,ets_smapes_mean,ets_smapes_med,ets_var_allerrors,ets_mean_allvar\\\n",
    "    =cross_validation_expanding_window_local(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'ExponentialSmoothing',ExponentialSmoothing)\n",
    "    \n",
    "    theta_time,theta_smapes_mean,theta_smapes_med,theta_var_allerrors,theta_mean_allvar\\\n",
    "    =cross_validation_expanding_window_local(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'Theta', Theta, theta=1.5,season_mode=SeasonalityMode.ADDITIVE)\n",
    "    \n",
    "    \n",
    "    smapes_mean = {\n",
    "    \"naive-last\": naive1_smapes_mean,\n",
    "    \"naive-seasonal\": naive12_smapes_mean,\n",
    "    \"Exponential Smoothing\": ets_smapes_mean,\n",
    "    \"Theta\": theta_smapes_mean\n",
    "    }\n",
    "    \n",
    "    smapes_med = {\n",
    "    \"naive-last\": naive1_smapes_med,\n",
    "    \"naive-seasonal\": naive12_smapes_med,\n",
    "    \"Exponential Smoothing\": ets_smapes_med,\n",
    "    \"Theta\": theta_smapes_med\n",
    "    }\n",
    "\n",
    "    elapsed_times = {\n",
    "    \"naive-last\": naive1_time,\n",
    "    \"naive-seasonal\": naive12_time,\n",
    "    \"Exponential Smoothing\": ets_time,\n",
    "    \"Theta\": theta_time\n",
    "    }\n",
    "    \n",
    "    # We compare models with respect to precision and computational cost.\n",
    "    print('Visualization of performances taking the mean of the smapes over cross-validation')\n",
    "    plot_models(elapsed_times, smapes_mean)\n",
    "    \n",
    "    print('Visualization of performances taking the median of the smapes over cross-validation')\n",
    "    plot_models(elapsed_times, smapes_med)\n",
    "    \n",
    "    return naive1_smapes_mean,naive12_smapes_mean,ets_smapes_mean,theta_smapes_mean,naive1_var_allerrors,naive1_mean_allvar,naive12_var_allerrors,naive12_mean_allvar,ets_var_allerrors,ets_mean_allvar,theta_var_allerrors,theta_mean_allvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indices=['index_cit_t1_t2','index_cit_t2_t1','index_colab_increm','index_colab_notincrem','index_keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Naive seasonal (1)\",\"Naive seasonal (12)\", \"Exponential smoothing\",\"Theta\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2': [12., 70., 30., 20.,1,2,3,4,5,6,7,8], 'Index citations t2-t1': [12., 70., 30., 20.,1,2,3,4,5,6,7,8],\n",
    "                   'Index colaboration incremental': [12., 70., 30., 20.,1,2,3,4,5,6,7,8],'colaboration not incremental': [12., 70., 30., 20.,1,2,3,4,5,6,7,8],\n",
    "                   'Index keywords': [12., 70., 30., 20.,1,2,3,4,5,6,7,8]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db5bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_table = []\n",
    "info_table_var=[]\n",
    "\n",
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    listindex_byalgo=[]\n",
    "    \n",
    "    list_var_byalgo=[]\n",
    "    \n",
    "    for horizon in [3,6,12]:\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        \n",
    "        naive1_smapes_mean,naive12_smapes_mean,ets_smapes_mean,theta_smapes_mean,naive1_var_allerrors,naive1_mean_allvar,\\\n",
    "        naive12_var_allerrors,naive12_mean_allvar,ets_var_allerrors,ets_mean_allvar,theta_var_allerrors,theta_mean_allvar=\\\n",
    "        local_forecasting(dfofindex,horizon,list_concepts,5)\n",
    "        \n",
    "        listindex_byalgo =listindex_byalgo+\\\n",
    "        [naive1_smapes_mean,naive12_smapes_mean,ets_smapes_mean,theta_smapes_mean]\n",
    "        \n",
    "        list_var_byalgo=list_var_byalgo+[[naive1_var_allerrors,naive1_mean_allvar],[naive12_var_allerrors,naive12_mean_allvar]      \n",
    "        ,[ets_var_allerrors,ets_mean_allvar],[theta_var_allerrors,theta_mean_allvar]]\n",
    "        \n",
    "    list_toappend=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend.append(listindex_byalgo[i+j*4])\n",
    "    info_table.append(list_toappend)\n",
    "    \n",
    "    list_toappend_var=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend_var.append(list_var_byalgo[i+j*4])\n",
    "    info_table_var.append(list(chain.from_iterable(list_toappend_var))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Naive seasonal (1)\",\"Naive seasonal (12)\", \"Exponential smoothing\",\"Theta\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table[0], 'Index citations t2-t1': info_table[1],\n",
    "                   'Index colaboration incremental': info_table[2],'colaboration not incremental':info_table[3],\n",
    "                   'Index keywords': info_table[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Naive seasonal (1)\",\"Naive seasonal (12)\", \"Exponential smoothing\",\"Theta\"], [\"3 months\", \"6 months\",\"12 months\"],[\"Mean of variances\", \"Variances over all errors\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\",\"Type of variance\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table_var[0], 'Index citations t2-t1': info_table_var[1],\n",
    "                   'Index colaboration incremental': info_table_var[2],'colaboration not incremental':info_table_var[3],\n",
    "                   'Index keywords': info_table_var[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f2687",
   "metadata": {},
   "source": [
    "The naive model with K=1 is the one which perform the best. This is probably due to the fact, that a very high percent of time series (~25%) are flat and therefore predicting the last values is very accurate if not completely exact in these cases. Besides, the time series are very chaotic. For this reason, even if the other forecasting algorithms might model the overall tendency of the time series, they fail to be very accurate and this results in big errors. Nevertheless, the exponential algorithm performs almost as well as the naive K=1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d448ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51d3e264",
   "metadata": {},
   "source": [
    "# **Part 2 -- Global forecasting based on clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0936f",
   "metadata": {},
   "source": [
    "We now do global forecasting on our time series using the clustering I created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c9a77",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "2.1. Randomized clustering of the time series.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bc5b2",
   "metadata": {},
   "source": [
    "To see whether the clustering makes sense to forecasting we will attribute randomly each sample to the same number of clusters as present in the algorithm, do the forecasting and compare the results. We should keep in mind, that there are 1+numbercluster number of clusters, since I created artifically a cluster with all the time series that are very flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_df_rand(dftm,list_concepts,numbercluster):\n",
    "\n",
    "    # creating a randomized clustering\n",
    "    \n",
    "    listcluster_rand=[]\n",
    "\n",
    "    for concept1 in list_concepts:\n",
    "        dfinter1=df_interpolation.loc[df_interpolation['concept1']==concept1]\n",
    "        for concept2 in list_concepts:\n",
    "            dfinter2=dfinter1.loc[dfinter1['concept2']==concept2]\n",
    "            interpolation_score = dfinter2.interpolation_ratio.tolist()[0]\n",
    "            for index in list_indices:\n",
    "                if interpolation_score<0.5:\n",
    "                    cluster_numbering= random.randint(1, numbercluster)\n",
    "                    listcluster_rand.append(cluster_numbering)\n",
    "    \n",
    "    #Creating the dataframe with randomized cluster.\n",
    "    dftm_rand=dftm.copy()\n",
    "    dftm_rand['cluster']=listcluster_rand\n",
    "    \n",
    "    return dftm_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcc882",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "2.2. Forecasting and comparing the results between the randomized and algorithmic clustering\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54542013",
   "metadata": {},
   "source": [
    "Comparing the different forecasting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_kcross_forecasts_cluster(smapes,modelname,addinfo):\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    themedians = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(smapes)):\n",
    "        themedians.append(np.median(smapes[i]))\n",
    "        #plt.hist(smapes[i], bins=100,label='K-cross set'+str(i+1))\n",
    "        labels = labels +['Trained and forecasted on the cluster '+str(i)+\\\n",
    "        ' , with a median smape of '+ str(round(np.median(smapes[i]),2))+' %.']\n",
    "        \n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"sMAPE\")\n",
    "    plt.title(\"The median sMAPE for \"+str(modelname)+' '+str(addinfo)+\" : %.3f\" % np.median(themedians))\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    \n",
    "    plt.hist(smapes,bins=100,histtype='barstacked',label=labels)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce448d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_splitting_window_global_clustering(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes=[]\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =maximal_train_len//number_cv\n",
    "            startingtime = i*windowslength\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len-(number_cv-1)*(maximal_train_len//number_cv) # to get the remaining window\n",
    "            startingtime = i*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "        \n",
    "        allsmapes.append(smapes)\n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "                \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "        \n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "    \n",
    "    \n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),allsmapes,np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_expanding_window_global_clustering(data,horizon,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    time1=time.time()\n",
    "    \n",
    "    startingtime=0\n",
    "    maximal_train_len = 252-horizon\n",
    "    score_forecaster = []\n",
    "    allsmapes=[]\n",
    "    \n",
    "    mean_allvar= []\n",
    "    var_alltheerrors = []\n",
    "    \n",
    "    for i in range(number_cv):\n",
    "        if i<number_cv-1:\n",
    "            windowslength =(i+1)*(maximal_train_len//number_cv)\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "            \n",
    "        else:\n",
    "            windowslength = maximal_train_len\n",
    "            df = creating_train_test_tm(data,startingtime,windowslength,horizon,list_concepts)\n",
    "            smapes,var_allerrors,var_mean_var\\\n",
    "            =eval_global_model(df['Train tm'].tolist(),df['Test tm'].tolist(),horizon,forecaster,**kwargs)\n",
    "        \n",
    "        allsmapes.append(smapes)\n",
    "        score_forecaster.append(np.median(smapes))\n",
    "        # I save the mean of the all the smapes done over one window\n",
    "        \n",
    "        var_alltheerrors.append(var_allerrors)\n",
    "        mean_allvar.append(var_mean_var)\n",
    "\n",
    "    myscores =np.array(score_forecaster)\n",
    "    \n",
    "    time2=time.time()\n",
    "    elapsed_time = time2-time1\n",
    "    \n",
    "    plt.plot()\n",
    "\n",
    "    return elapsed_time,np.mean(myscores),np.median(myscores),allsmapes,np.mean(var_allerrors),np.mean(mean_allvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559021f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_computation(dftm, dftm_rand,horizon,numbercluster,list_concepts,number_cv,modelname,forecaster,**kwargs):\n",
    "    \n",
    "    elapsed_time_rand=[]\n",
    "    error_rand_mean=[]\n",
    "    error_rand_med=[]\n",
    "    var_allerrors_rand = []\n",
    "    mean_allvar_rand =[]\n",
    "    \n",
    "    allsmapes_rand = []\n",
    "    # once I created my dataframe with this specific horizon I train and visualize the results of the forecasting algorithms\n",
    "\n",
    "    print('Randomized clustering')\n",
    "    for i in tqdm(range(1,numbercluster+1)):\n",
    "\n",
    "        mydf_rand= dftm_rand.loc[dftm_rand['cluster']==int(i)]\n",
    "        \n",
    "        time_rand, smapes_rand_mean,smapes_rand_med,smapes_rand,var_allerrors_rand_number,mean_allvar_rand_number =\\\n",
    "        cross_validation_expanding_window_global_clustering(mydf_rand,\\\n",
    "        horizon,list_concepts,number_cv,modelname,forecaster,**kwargs)\n",
    "\n",
    "        elapsed_time_rand.append(time_rand)\n",
    "        error_rand_mean.append(smapes_rand_mean)\n",
    "        error_rand_med.append(smapes_rand_med)\n",
    "        allsmapes_rand.append(list(chain.from_iterable(smapes_rand)))\n",
    "        \n",
    "        var_allerrors_rand.append(var_allerrors_rand_number)\n",
    "        mean_allvar_rand.append(mean_allvar_rand_number)\n",
    "\n",
    "    \n",
    "    eval_kcross_forecasts_cluster(allsmapes_rand,modelname,'randomized')\n",
    "    \n",
    "        \n",
    "    elapsed_time=[]\n",
    "    error_mean=[]\n",
    "    error_med=[]\n",
    "    var_allerrors = []\n",
    "    mean_allvar =[]\n",
    "    \n",
    "    allsmapes = []\n",
    "    \n",
    "    print('Algorithmic clustering based on similarity')\n",
    "    for i in tqdm(range(1,numbercluster+1)):     \n",
    "\n",
    "        mydf= dftm.loc[dftm['cluster']==int(i)]\n",
    "        \n",
    "        time_normal, smapes_mean,smapes_med,smapes_normal,var_allerrors_number,mean_allvar_number =\\\n",
    "        cross_validation_expanding_window_global_clustering(mydf,\\\n",
    "        horizon,list_concepts,number_cv,modelname,forecaster,**kwargs)\n",
    "        \n",
    "        elapsed_time.append(time_normal)\n",
    "        error_mean.append(smapes_mean)\n",
    "        error_med.append(smapes_med)\n",
    "        allsmapes.append(list(chain.from_iterable(smapes_normal)))\n",
    "          \n",
    "        var_allerrors.append(var_allerrors_number)\n",
    "        mean_allvar.append(mean_allvar_number)\n",
    "    \n",
    "    eval_kcross_forecasts_cluster(allsmapes,modelname,'not-randomized')\n",
    "    \n",
    "    smapes_rand = np.mean(error_rand_mean)\n",
    "    time_rand = np.mean(elapsed_time_rand)\n",
    "    smapes = np.mean(error_mean)\n",
    "    time = np.mean(elapsed_time)\n",
    "    \n",
    "    return smapes_rand,time_rand,smapes,time,np.mean(var_allerrors_rand),np.mean(mean_allvar_rand),np.mean(var_allerrors),np.mean(mean_allvar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_forecasting(dftm,dftm_rand,horizon,numbercluster,list_concepts,number_cv):\n",
    "    mylag =252//number_cv-2*horizon\n",
    "    # number of datapoints considered to forecast\n",
    "    # I substracted -3, because I was having problems with the authorized lags in the function  \n",
    "\n",
    "    print(' ')\n",
    "    \n",
    "    lr_smapes_rand,lr_time_rand,lr_smapes,lr_time,\\\n",
    "    lr_var_allerrors_rand,lr_mean_allvar_rand,lr_var_allerrors,lr_mean_allvar\\\n",
    "    = cluster_computation(dftm, dftm_rand,horizon,numbercluster,\\\n",
    "    list_concepts,number_cv,'LinearRegressionModel',LinearRegressionModel, lags=mylag, output_chunk_length=horizon)\n",
    "        \n",
    "    print(' ')\n",
    "        \n",
    "    lgbm_smapes_rand,lgbm_time_rand,lgbm_smapes,lgbm_time,\\\n",
    "    lgbm_var_allerrors_rand,lgbm_mean_allvar_rand,lgbm_var_allerrors,lgbm_mean_allvar\\\n",
    "    = cluster_computation(dftm, dftm_rand,horizon,numbercluster,\\\n",
    "    list_concepts,number_cv,'LightGBMModel',LightGBMModel, lags=mylag, output_chunk_length=horizon, objective=\"mape\")\n",
    "    \n",
    "    print(' ')\n",
    "    \n",
    "    rf_smapes_rand,rf_time_rand,rf_smapes,rf_time, \\\n",
    "    rf_var_allerrors_rand,rf_mean_allvar_rand,rf_var_allerrors,rf_mean_allvar\\\n",
    "    = cluster_computation(dftm, dftm_rand,horizon,numbercluster,\\\n",
    "    list_concepts,number_cv,'RandomForest',RandomForest,lags=mylag, output_chunk_length=horizon)\n",
    "    \n",
    "    print(' ')\n",
    "    \n",
    "    nbeat_smapes_rand,nbeat_time_rand,nbeat_smapes,nbeat_time,\\\n",
    "    nbeat_var_allerrors_rand,nbeat_mean_allvar_rand,nbeat_var_allerrors,nbeat_mean_allvar\\\n",
    "    = cluster_computation(dftm, dftm_rand,horizon,numbercluster,\\\n",
    "    list_concepts,number_cv,'NBEATSModel',NBEATSModel,input_chunk_length=mylag,output_chunk_length=horizon,num_stacks=20,\n",
    "    num_blocks=1,num_layers=2,layer_widths=136,expansion_coefficient_dim=11,loss_fn=SmapeLoss(),batch_size=1024,\n",
    "    optimizer_kwargs={\"lr\":  1e-3})\n",
    "    \n",
    "    # visualization of the results \n",
    "\n",
    "    \n",
    "    smapes_2 = {\n",
    "        \"Linear Regression\": lr_smapes,\n",
    "        \"LGBM\": lgbm_smapes,\n",
    "        \"Random Forest\": rf_smapes,\n",
    "        \"NBeats\": nbeat_smapes,\n",
    "        \n",
    "        \"Linear Regression_rand\": lr_smapes_rand,\n",
    "        \"LGBM_rand\": lgbm_smapes_rand,\n",
    "        \"Random Forest_rand\": rf_smapes_rand,\n",
    "        \"NBeats_rand\": nbeat_smapes_rand,\n",
    "    }\n",
    "\n",
    "    elapsed_times_2 = {\n",
    "        \"Linear Regression\": lr_time,\n",
    "        \"LGBM\": lgbm_time,\n",
    "        \"Random Forest\": rf_time,\n",
    "        \"NBeats\": nbeat_time,\n",
    "        \n",
    "        \"Linear Regression_rand\": lr_time_rand,\n",
    "        \"LGBM_rand\": lgbm_time_rand,\n",
    "        \"Random Forest_rand\": rf_time_rand,\n",
    "        \"NBeats_rand\": nbeat_time_rand,\n",
    "    }\n",
    "\n",
    "    plot_models_cluster(elapsed_times_2, smapes_2)\n",
    "    \n",
    "    list_var_error = [[nbeat_var_allerrors_rand,nbeat_mean_allvar_rand,nbeat_var_allerrors,nbeat_mean_allvar],\n",
    "            [rf_var_allerrors_rand,rf_mean_allvar_rand,rf_var_allerrors,rf_mean_allvar],\n",
    "            [lgbm_var_allerrors_rand,lgbm_mean_allvar_rand,lgbm_var_allerrors,lgbm_mean_allvar],\n",
    "            [lr_var_allerrors_rand,lr_mean_allvar_rand,lr_var_allerrors,lr_mean_allvar]]\n",
    "    \n",
    "    return lr_smapes,lgbm_smapes,rf_smapes,nbeat_smapes,lr_smapes_rand,lgbm_smapes_rand,rf_smapes_rand,nbeat_smapes_rand,list_var_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcf9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbercluster = max(dftm.cluster.tolist())\n",
    "numbercluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm_rand=clustering_df_rand(dftm,list_concepts,numbercluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2c865",
   "metadata": {},
   "source": [
    "We just do without cross-validation, because with cross validation it is way to long. At least this can give us a reasonable idea of the performance of clustering forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58198698",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = []\n",
    "\n",
    "info_table_var = []\n",
    "\n",
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    listindex_byalgo=[]\n",
    "    dfofindex_rand = dftm_rand.loc[dftm_rand['indextype']==index]\n",
    "    \n",
    "    listvar_byalgo =[]\n",
    "    \n",
    "    for horizon in [3]:\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        lr_smapes,lgbm_smapes,rf_smapes,nbeat_smapes,lr_smapes_rand,lgbm_smapes_rand,rf_smapes_rand,nbeat_smapes_rand,\\\n",
    "        list_var_error=clustering_forecasting(dfofindex,dfofindex_rand,horizon,numbercluster,list_concepts,1)\n",
    "        \n",
    "        listindex_byalgo=listindex_byalgo+\\\n",
    "        [lr_smapes,lgbm_smapes,rf_smapes,nbeat_smapes,lr_smapes_rand,lgbm_smapes_rand,\\\n",
    "        rf_smapes_rand,nbeat_smapes_rand]\n",
    "        \n",
    "        listvar_byalgo = listvar_byalgo+list_var_error\n",
    "    \n",
    "    list_toappend=[]    \n",
    "    for i in range(8):\n",
    "        for j in range(3):\n",
    "            list_toappend.append(listindex_byalgo[i+j*8])\n",
    "    info_table.append(list_toappend)\n",
    "    \n",
    "    list_toappend_var=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend_var.append(listvar_byalgo[i+j*4])\n",
    "    info_table_var.append(list(chain.from_iterable(list_toappend_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc2f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d14b5ff",
   "metadata": {},
   "source": [
    "not randomized clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table[0][:12], 'Index citations t2-t1': info_table[1][:12],\n",
    "                   'Index colaboration incremental': info_table[2][:12],'colaboration not incremental':info_table[3][:12],\n",
    "                   'Index keywords': info_table[4][:12]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26351c",
   "metadata": {},
   "source": [
    "randomized clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table[0][12:], 'Index citations t2-t1': info_table[1][12:],\n",
    "                   'Index colaboration incremental': info_table[2][12:],'colaboration not incremental':info_table[3][12:],\n",
    "                   'Index keywords': info_table[4][12:]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"],[\"Randomized\",\"Not-randomized\"],[\"Mean of variances\", \"Variances over all errors\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\",\"Randomization\",\"Type of variance\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table_var[0], 'Index citations t2-t1': info_table_var[1],\n",
    "                   'Index colaboration incremental': info_table_var[2],'colaboration not incremental':info_table_var[3],\n",
    "                   'Index keywords': info_table_var[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6794c",
   "metadata": {},
   "source": [
    "As a conclusion, we observe that Nbeats and Random forest are the best forecasting algorithms in the setting of clusters forecasting. For a horizon 3 and 6, random forest is better while for a horizon 12, nbeats is slightly better than random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0707f38",
   "metadata": {},
   "source": [
    "# **Part 3 -- Global forecasting based on my own data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469edff",
   "metadata": {},
   "source": [
    "This takes 12 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017738fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_forecasting_mydata(dftm,horizon,list_concepts,number_cv):\n",
    "\n",
    "    mylag =252//number_cv-2*horizon\n",
    "\n",
    "    # regression models\n",
    "            \n",
    "    lr_time, lr_smapes_mean,lr_smapes_med,lr_var_allerrors,lr_mean_allvar =\\\n",
    "    cross_validation_expanding_window_global(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'LinearRegressionModel', LinearRegressionModel, lags=mylag, output_chunk_length=horizon)\n",
    "    \n",
    "    lgbm_time, lgbm_smapes_mean,lgbm_smapes_med,lgbm_var_allerrors,lgbm_mean_allvar =\\\n",
    "    cross_validation_expanding_window_global(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'LightGBMModel', LightGBMModel,lags=mylag, output_chunk_length=horizon, objective=\"mape\")\n",
    "  \n",
    "    rf_time, rf_smapes_mean,rf_smapes_med,rf_var_allerrors,rf_mean_allvar\\\n",
    "    =cross_validation_expanding_window_global(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'RandomForest', RandomForest,lags=mylag, output_chunk_length=horizon)\n",
    "    \n",
    "    #deep learning models\n",
    "\n",
    "    nbeat_time,nbeat_smapes_mean,nbeat_smapes_med,nbeat_var_allerrors,nbeat_mean_allvar\\\n",
    "    =cross_validation_expanding_window_global(dftm,\\\n",
    "    horizon,list_concepts,number_cv,'NBEATSModel', NBEATSModel,input_chunk_length=mylag,output_chunk_length=horizon,num_stacks=20,\n",
    "    num_blocks=1,num_layers=2,layer_widths=136,expansion_coefficient_dim=11,loss_fn=SmapeLoss(),batch_size=1024,\n",
    "    optimizer_kwargs={\"lr\":  1e-3})\n",
    "    \n",
    "    smapes_mean = {\n",
    "        \"Linear Regression\": lr_smapes_mean,\n",
    "        \"LGBM\": lgbm_smapes_mean,\n",
    "        \"Random Forest\": rf_smapes_mean,\n",
    "        \"NBeats\": nbeat_smapes_mean,\n",
    "    }\n",
    "\n",
    "    elapsed_times = {\n",
    "        \"Linear Regression\": lr_time,\n",
    "        \"LGBM\": lgbm_time,\n",
    "        \"Random Forest\": rf_time,\n",
    "        \"NBeats\": nbeat_time,\n",
    "    }\n",
    "\n",
    "    smapes_med = {\n",
    "        \"Linear Regression\": lr_smapes_med,\n",
    "        \"LGBM\": lgbm_smapes_med,\n",
    "        \"Random Forest\": rf_smapes_med,\n",
    "        \"NBeats\": nbeat_smapes_med,\n",
    "    }\n",
    "\n",
    "\n",
    "        \n",
    "    print('Visualization of performances taking the mean of the smapes over cross-validation')\n",
    "    plot_models(elapsed_times, smapes_mean)\n",
    "    \n",
    "        \n",
    "    print('Visualization of performances taking the median of the smapes over cross-validation')\n",
    "    plot_models(elapsed_times, smapes_med)\n",
    "    \n",
    "    return lr_smapes_mean,lgbm_smapes_mean,rf_smapes_mean,nbeat_smapes_mean,\n",
    "    lr_var_allerrors,lr_mean_allvar,lgbm_var_allerrors,lgbm_mean_allvar,\n",
    "    rf_var_allerrors,rf_mean_allvar,nbeat_var_allerrors,nbeat_mean_allvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = []\n",
    "info_table_var=[]\n",
    "\n",
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    listindex_byalgo=[]\n",
    "\n",
    "    list_var_byalgo=[]\n",
    "    \n",
    "    for horizon in [3,6,12]:\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        lr_smapes_mean,lgbm_smapes_mean,rf_smapes_mean,nbeats_smapes_mean,\\\n",
    "        lr_var_allerrors,lr_mean_allvar,lgbm_var_allerrors,lgbm_mean_allvar,\\\n",
    "        rf_var_allerrors,rf_mean_allvar,nbeat_var_allerrors,nbeat_mean_allvar=\\\n",
    "        global_forecasting_mydata(dfofindex,horizon,list_concepts,5)\n",
    "        \n",
    "        listindex_byalgo=listindex_byalgo+\\\n",
    "        [lr_smapes_mean,lgbm_smapes_mean,rf_smapes_mean,nbeats_smapes_mean]\n",
    "        \n",
    "        list_var_byalgo=list_var_byalgo+[[lr_var_allerrors,lr_mean_allvar],[lgbm_var_allerrors,lgbm_mean_allvar],\n",
    "        [rf_var_allerrors,rf_mean_allvar],[nbeat_var_allerrors,nbeat_mean_allvar]]\n",
    "    \n",
    "    list_toappend=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend.append(listindex_byalgo[i+j*4])\n",
    "    info_table.append(list_toappend)\n",
    "    \n",
    "    list_toappend_var=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend_var.append(list_var_byalgo[i+j*4])\n",
    "    info_table_var.append(list(chain.from_iterable(list_toappend_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e0cae",
   "metadata": {},
   "source": [
    "Global forecasting table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb0336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table[0], 'Index citations t2-t1': info_table[1],\n",
    "                   'Index colaboration incremental': info_table[2],'colaboration not incremental':info_table[3],\n",
    "                   'Index keywords': info_table[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1e0f5",
   "metadata": {},
   "source": [
    "We see that the random forest algorithm outperforms the other algorithm and becomes only slightly less accurate with the increase of the forecasting window. Nevertheless, the forecasting based on clustering using random forests (or nBeats) is significantly better than the insample forecasting as presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"],[\"Mean of variances\", \"Variances over all errors\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\",\"Type of variance\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table_var[0], 'Index citations t2-t1': info_table_var[1],\n",
    "                   'Index colaboration incremental': info_table_var[2],'colaboration not incremental':info_table_var[3],\n",
    "                   'Index keywords': info_table_var[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04f69e",
   "metadata": {},
   "source": [
    "# **Part 4 -- Global forecasting based on transfer learning methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec55ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylag=252-2*horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8652d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_m4(horizon,mytype='scaled') -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
    "    \n",
    "    # load TimeSeries - the splitting and scaling has already been done\n",
    "    if mytype =='scaled':\n",
    "        print(\"loading M4 TimeSeries...\")\n",
    "        with open(\"m4_monthly_scaled.pkl\", \"rb\") as f:\n",
    "            m4_series = pickle.load(f)\n",
    "        \n",
    "    else:\n",
    "        print(\"loading M4 TimeSeries...\")\n",
    "        with open(\"m4_monthly.pkl\", \"rb\") as f:\n",
    "            m4_series = pickle.load(f)\n",
    "\n",
    "    # filter and keep only series that contain at least 48 training points\n",
    "    m4_series = list(filter(lambda t: len(t[0]) >= 252, m4_series))\n",
    "\n",
    "    list_timeseries = []\n",
    "    for tm in m4_series:\n",
    "        mylist = []\n",
    "        for x in tm[0].all_values():\n",
    "            mylist.append(x[0][0])\n",
    "    \n",
    "        list_goodtype = np.array(mylist).astype(float)    \n",
    "        mytm =TimeSeries.from_values(list_goodtype)\n",
    "        list_timeseries.append(mytm)\n",
    "\n",
    "\n",
    "    m4_train = [s[:-horizon] for s in list_timeseries]\n",
    "    m4_test = [s[-horizon:] for s in list_timeseries]\n",
    "\n",
    "    print(\n",
    "        \"done. There are {} series, with average training length {}\".format(\n",
    "            len(m4_train), np.mean([len(s) for s in list_timeseries])\n",
    "        ))\n",
    "    print(\n",
    "        \"The longest time series contain {} datapoints and the shortes time series {} datapoints.\".format(\n",
    "            np.max([len(s) for s in m4_train]), np.min([len(s) for s in list_timeseries])\n",
    "        )\n",
    "    )\n",
    "    return m4_train, m4_test ,list_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b998ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbeats_transferlearning(external_data,train_data,test_data,horizon,mylag):\n",
    "\n",
    "    # Slicing hyper-params:\n",
    "    IN_LEN = mylag\n",
    "    OUT_LEN = horizon\n",
    "\n",
    "    # Architecture hyper-params:\n",
    "    NUM_STACKS = 20\n",
    "    NUM_BLOCKS = 1\n",
    "    NUM_LAYERS = 2\n",
    "    LAYER_WIDTH = 136\n",
    "    COEFFS_DIM = 11\n",
    "\n",
    "    # Training settings:\n",
    "    LR = 1e-3\n",
    "    BATCH_SIZE = 1024\n",
    "    MAX_SAMPLES_PER_TS = (\n",
    "    10  # <-- new parameter, limiting the number of training samples per series\n",
    "    )\n",
    "    NUM_EPOCHS = 5\n",
    "\n",
    "    # reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    nbeats_model_m4 = NBEATSModel(\n",
    "    input_chunk_length=IN_LEN,\n",
    "    output_chunk_length=OUT_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_stacks=NUM_STACKS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    layer_widths=LAYER_WIDTH,\n",
    "    expansion_coefficient_dim=COEFFS_DIM,\n",
    "    loss_fn=SmapeLoss(),\n",
    "    optimizer_kwargs={\"lr\": LR},\n",
    "    #pl_trainer_kwargs={\n",
    "     #   \"enable_progress_bar\": True,\n",
    "     #   \"accelerator\": \"gpu\",\n",
    "     #   \"gpus\": -1,\n",
    "     #   \"auto_select_gpus\": True,\n",
    "    #},\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    nbeats_model_m4.fit(\n",
    "    external_data,\n",
    "    num_loader_workers=4,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    preds = nbeats_model_m4.predict(series=train_data, n=horizon)  # get forecasts\n",
    "    nbeats_m4_elapsed_time = time.time() - start_time\n",
    "    \n",
    "    nbeats_m4_smapes,nbeats_m4_var_allerrors,nbeats_m4_var_mean_var = mysmape(preds, test_data)\n",
    "    eval_kcross_forecasts(nbeats_m4_smapes,modelname)\n",
    "    \n",
    "    return nbeats_m4_smapes,nbeats_m4_elapsed_time,nbeats_m4_var_allerrors,nbeats_m4_var_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_forecasting_transferlearning(external_data,dftm_horizon,horizon,list_concepts,mylag):\n",
    "    \n",
    "    # number of datapoints considered to forecast\n",
    "    # I substracted -3, because I was having problems with the authorized lags in the function\n",
    "    \n",
    "    # regression models\n",
    "\n",
    "    #model 1\n",
    "    time1=time.time()\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    lr_model_m4 = LinearRegressionModel(lags=mylag, output_chunk_length=horizon)\n",
    "    lr_model_m4.fit(external_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    preds = lr_model_m4.predict(n=horizon, series=dftm_horizon['Train tm'].tolist())\n",
    "    lr_time_transfer = time.time() - tic\n",
    "    \n",
    "    lr_smapes_transfer,lr_var_allerrors,lr_var_mean_var =mysmape(preds,dftm_horizon['Test tm'].tolist())\n",
    "    eval_kcross_forecasts(lr_smapes_transfer,'LinearRegressionModel')\n",
    "    \n",
    "    time2=time.time()\n",
    "    \n",
    "    print('It took '+str(time2-time1)+' seconds to fit and forecast with LinearRegressionModel')\n",
    "    \n",
    "    #model 2\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    lgbm_model_m4 = LightGBMModel(lags=mylag, output_chunk_length=horizon, objective=\"mape\")\n",
    "    lgbm_model_m4.fit(external_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    preds = lgbm_model_m4.predict(n=horizon, series=dftm_horizon['Train tm'].tolist())\n",
    "    lgbm_time_transfer = time.time() - tic\n",
    "    \n",
    "    lgbm_smapes_transfer,lgbm_var_allerrors,lgbm_var_mean_var =mysmape(preds,dftm_horizon['Test tm'].tolist())\n",
    "    eval_kcross_forecasts(lgbm_smapes_transfer,'LightGBMModel')\n",
    "    \n",
    "    \n",
    "    time3=time.time()\n",
    "    \n",
    "    print('It took '+str(time3-time2)+' seconds to fit and forecast with LightGBMModel')\n",
    "    \n",
    "    # model 3 \n",
    "    \n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    randomforest = RandomForest(lags=mylag, output_chunk_length=horizon)\n",
    "    randomforest.fit(external_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    preds = randomforest.predict(n=horizon, series=dftm_horizon['Train tm'].tolist())\n",
    "    rf_time_transfer = time.time() - tic\n",
    "    \n",
    "    rf_smapes_transfer,rf_var_allerrors,rf_var_mean_var=mysmape(preds,dftm_horizon['Test tm'].tolist())\n",
    "    eval_kcross_forecasts(rf_smapes_transfer,'randomforest')\n",
    "    \n",
    "    \n",
    "    time4=time.time()\n",
    "    \n",
    "    print('It took '+str(time4-time3)+' seconds to fit and forecast with randomforest')\n",
    "    \n",
    "    #deep learning models\n",
    "    \n",
    "    nbeats_m4_smapes,nbeats_m4_elapsed_time,nbeats_m4_var_allerrors,nbeats_m4_var_mean_var\\\n",
    "    = nbeats_transferlearning(external_data,dftm_horizon['Train tm'].tolist(), dftm_horizon['Test tm'].tolist(),horizon)\n",
    "    \n",
    "    time5=time.time()\n",
    "    \n",
    "    print('It took '+str(time5-time4)+' seconds to fit and forecast with nbeats')\n",
    "\n",
    "    smapes_2 = {\n",
    "        \"Linear Regression\": lr_smapes_transfer,\n",
    "        \"LGBM\": lgbm_smapes_transfer,\n",
    "        \"Random Forest\": rf_smapes_transfer,\n",
    "        \"NBeats\": nbeats_m4_smapes,\n",
    "    }\n",
    "\n",
    "    elapsed_times_2 = {\n",
    "        \"Linear Regression\": lr_time_transfer,\n",
    "        \"LGBM\": lgbm_time_transfer,\n",
    "        \"Random Forest\": rf_time_transfer,\n",
    "        \"NBeats\": nbeats_m4_elapsed_time,\n",
    "    }\n",
    "\n",
    "    plot_models(elapsed_times_2, smapes_2)\n",
    "    \n",
    "    list_var = [lr_var_allerrors,lr_var_mean_var,lgbm_var_allerrors,lgbm_var_mean_var,\n",
    "                rf_var_allerrors,rf_var_mean_var,nbeats_m4_var_allerrors,nbeats_m4_var_mean_var]\n",
    "    \n",
    "    return lr_smapes_transfer,lgbm_smapes_transfer,rf_smapes_transfer,nbeats_m4_smapes,list_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780232a",
   "metadata": {},
   "source": [
    "We first run the forecasting algorithm with a part of the training data, to see what are the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f152ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_train_scaled, m4_test_scaled, m4_scaled_all= load_m4(5,'scaled')\n",
    "m4_train_notscaled, m4_test_notscaled, m4_notscaled_all = load_m4(5,'notscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef557b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftm_horizon = creating_train_test_tm(dfofindex,0,85,horizon,list_concepts)\n",
    "dftm_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1f4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc51419",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = []\n",
    "info_table_var=[]\n",
    "\n",
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    listindex_byalgo=[]\n",
    "    \n",
    "    listvar_byalgo =[]\n",
    "    \n",
    "    for horizon in [3,6,12]:\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        dftm_horizon = creating_train_test_tm(dfofindex,0,252-horizon,horizon,list_concepts)\n",
    "\n",
    "        external_data = m4_notscaled_all[:5000]\n",
    "        \n",
    "        lr_smapes_transfer,lgbm_smapes_transfer,rf_smapes_transfer,nbeats_m4_smapes,list_var_error =\\\n",
    "        global_forecasting_transferlearning(external_data,dftm_horizon,horizon,list_concepts,150)\n",
    "        \n",
    "        listindex_byalgo=listindex_byalgo+[lr_smapes_transfer,lgbm_smapes_transfer,\\\n",
    "                                rf_smapes_transfer,nbeats_m4_smapes]\n",
    "        listvar_byalgo = listvar_byalgo+list_var_error\n",
    "    \n",
    "    list_toappend=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend.append(listindex_byalgo[i+j*4])\n",
    "    info_table.append(list_toappend)\n",
    "    \n",
    "    list_toappend_var=[]    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            list_toappend_var.append(listvar_byalgo[i+j*4])\n",
    "    info_table_var.append(list(chain.from_iterable(list_toappend_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa0d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17064bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table[0], 'Index citations t2-t1': info_table[1],\n",
    "                   'Index colaboration incremental': info_table[2],'colaboration not incremental':info_table[3],\n",
    "                   'Index keywords': info_table[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03438c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589dda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[\"Linear Regression\",\"LGBM\", \"Random Forest\",\"NBeats\"], [\"3 months\", \"6 months\",\"12 months\"],[\"Mean of variances\", \"Variances over all errors\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Forecaster algorithm\", \"Forecasting horizon\",\"Type of variance\"])\n",
    "\n",
    "df = pd.DataFrame({'Index citations t1-t2':info_table_var[0], 'Index citations t2-t1': info_table_var[1],\n",
    "                   'Index colaboration incremental': info_table_var[2],'colaboration not incremental':info_table_var[3],\n",
    "                   'Index keywords': info_table_var[4]}, index=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13103cb",
   "metadata": {},
   "source": [
    "One can see that Nbeats perform significantly better with respect to efficiency and error than all the other algorithms (linear Regression, LGBM, Random Forest) regardless of the horizon we choose. Nevertheless, it does not perform as well as the Random forest on clustered forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486f19d",
   "metadata": {},
   "source": [
    "Since Nbeats is the algorithm that perform the best, we check how well it works training all the data on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a536730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    for horizon in tqdm([3,6,12]):\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        dftm_horizon = creating_train_test_tm(dfofindex,0,252-horizon,horizon,list_concepts)\n",
    "        nbeats_transferlearning(m4_notscaled_all,dftm_horizon['Train tm'].tolist(), dftm_horizon['Test tm'].tolist(),\\\n",
    "        horizon,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5f6e9",
   "metadata": {},
   "source": [
    "Comment about performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e201c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in list_indices:\n",
    "    dfofindex = dftm.loc[dftm['indextype']==index]\n",
    "    print('The results for '+str(index)+' are:')\n",
    "    for horizon in tqdm([3,6,12]):\n",
    "        print('The results for '+str(index)+' with an horizon of '+str(horizon)+' are:')\n",
    "        dftm_horizon = creating_train_test_tm(dfofindex,0,252-horizon,horizon,list_concepts)\n",
    "        mylag =70\n",
    "    \n",
    "        time1=time.time()\n",
    "    \n",
    "        random.seed(42)\n",
    "\n",
    "        randomforest = RandomForest(lags=mylag, output_chunk_length=horizon)\n",
    "        randomforest.fit(m4_notscaled_all)\n",
    "\n",
    "        tic = time.time()\n",
    "        preds = randomforest.predict(n=horizon, series=dftm_horizon['Train tm'].tolist())\n",
    "        rf_time_transfer = time.time() - tic\n",
    "        \n",
    "        rf_smapes_transfer,rf_var_allerrors,rf_var_mean_var=mysmape(preds,dftm_horizon['Test tm'].tolist())\n",
    "        eval_kcross_forecasts(rf_smapes_transfer,'randomforest')\n",
    "    \n",
    "        time2=time.time()\n",
    "    \n",
    "        print('It took '+str(time2-time1)+' seconds to fit and forecast with randomforest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea814b42",
   "metadata": {},
   "source": [
    "# **Part 5 -- Fitting best model: hyperparameters optimization, visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_train_notscaled, m4_test_notscaled, m4_notscaled_all = load_m4(2,'notscaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    in_len = trial.suggest_int(\"in_len\", 12, 40)\n",
    "    out_len = trial.suggest_int(\"out_len\", 1, 6)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = LinearRegressionModel(lags=in_len, output_chunk_length=out_len)\n",
    "    \n",
    "    model.fit(m4_notscaled_all[:100])\n",
    "    \n",
    "    dftm_horizon = creating_train_test_tm(dftm,out_len,list_concepts)\n",
    "    \n",
    "    preds = model.predict(series=dftm_horizon['Train tm'], n=out_len)\n",
    "    smapes_transfer,var_allerrors,var_mean_var = mysmape(val, preds)\n",
    "    smape_val = np.mean(smapes)\n",
    "\n",
    "    return smape_val if smape_val != np.nan else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftm_horizon = creating_train_test_tm(dftm,40,list_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    # select input and output chunk lengths\n",
    "    in_len = trial.suggest_int(\"in_len\", 12, 40)\n",
    "    out_len = trial.suggest_int(\"out_len\", 1, in_len-1)\n",
    "\n",
    "    # throughout training we'll monitor the validation loss for both pruning and early stopping\n",
    "    pruner = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.001, patience=3, verbose=True)\n",
    "    callbacks = [pruner, early_stopper]\n",
    "\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # build the TCN model\n",
    "    \n",
    "    model = LinearRegressionModel(lags=in_len, output_chunk_length=out_len)\n",
    "    \n",
    "\n",
    "    # when validating during training, we can use a slightly longer validation\n",
    "    # set which also contains the first input_chunk_length time steps\n",
    "    #model_val_set = m4_notscaled_all[-(VAL_LEN + in_len) :]\n",
    "\n",
    "    # train the model\n",
    "    model.fit(\n",
    "        series=m4_notscaled_all[:100]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluate how good it is on the validation set, using sMAPE\n",
    "    preds = model.predict(series=dftm_horizon['Train tm'], n=VAL_LEN)\n",
    "    smapes_transfer,var_allerrors,var_mean_va = mysmape(val, preds)\n",
    "    smape_val = np.mean(smapes)\n",
    "\n",
    "    return smape_val if smape_val != np.nan else float(\"inf\")\n",
    "\n",
    "\n",
    "# for convenience, print some optimization trials information\n",
    "def print_callback(study, trial):\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparameters by minimizing the sMAPE on the validation set\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7e227",
   "metadata": {},
   "source": [
    "Try a probabilistic forecast with the model I created to see, if I could get a better forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_es = ExponentialSmoothing()\n",
    "#model_es.fit(train)\n",
    "#probabilistic_forecast = model_es.predict(len(val), num_samples=1000)\n",
    "\n",
    "#series.plot(label=\"actual\")\n",
    "#probabilistic_forecast.plot(label=\"probabilistic forecast\")\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a461357",
   "metadata": {},
   "source": [
    "Checking if the forecasting works well visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoARIMA()\n",
    "# model.fit(train)\n",
    "# forecast = model.predict(8)\n",
    "\n",
    "# series.plot()\n",
    "# forecast.plot(label=\"AutoARIMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396c753",
   "metadata": {},
   "source": [
    "Checking if the model is not overfitted (with historical_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_es = ExponentialSmoothing()\n",
    "#historical_fcast_es = model_es.historical_forecasts(\n",
    "#    series, start=0.6, forecast_horizon=3, verbose=True,num_samples=500\n",
    "#)\n",
    "\n",
    "#series.plot(label=\"data\")\n",
    "#historical_fcast_es.plot(label=\"backtest 3-months ahead forecast (Exp. Smoothing)\")\n",
    "#print(\"SMAPE = {:.2f}%\".format(smape(historical_fcast_es, series)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745a71e",
   "metadata": {},
   "source": [
    "To check could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7424a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from darts.models import TCNModel\n",
    "#from darts.utils.likelihood_models import LaplaceLikelihood\n",
    "\n",
    "#model = TCNModel(\n",
    "#    input_chunk_length=24,\n",
    "#    output_chunk_length=12,\n",
    "#    random_state=42,\n",
    "#    likelihood=LaplaceLikelihood(),\n",
    "#)\n",
    "\n",
    "#model.fit(trainnew, epochs=400, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66110538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8188582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
